{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81ee95c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: schedule in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 18:27:04,451 - INFO - ================================================================================\n",
      "2025-05-09 18:27:04,491 - INFO - DÉMARRAGE DU PROGRAMME DE COLLECTE DE DONNÉES SISMIQUES IPGP - 2025-05-09 18:27:04\n",
      "2025-05-09 18:27:04,503 - INFO - Base de données: NewDataseisme_corrige.csv\n",
      "2025-05-09 18:27:04,507 - INFO - Source des données: https://ws.ipgp.fr/fdsnws/event/1/query?minlatitude=-13.543702&maxlatitude=-12.398181&minlongitude=44.708653&maxlongitude=46.417027&minmagnitude=0&format=text&nodata=404\n",
      "2025-05-09 18:27:04,516 - INFO - Colonnes désirées: ['Date', 'Magnitude', 'Latitude', 'Longitude', 'Profondeur', 'origine']\n",
      "2025-05-09 18:27:04,523 - INFO - Heure de collecte quotidienne: 02:00\n",
      "2025-05-09 18:27:04,529 - INFO - ================================================================================\n",
      "2025-05-09 18:27:04,538 - INFO - Début de la collecte des nouvelles données sismiques\n",
      "2025-05-09 18:27:05,578 - INFO - Base de données existante chargée: 14216 lignes\n",
      "2025-05-09 18:27:05,622 - INFO - Exemples de dates dans la base existante: ['08/05/2025 20:34', '08/05/2025 20:27', '08/05/2025 19:16']\n",
      "2025-05-09 18:27:07,137 - INFO - Date la plus récente dans la base: 08/05/2025 20:34 (parsed as 2025-05-08 20:34:00)\n",
      "2025-05-09 18:27:07,187 - INFO - Téléchargement des données depuis https://ws.ipgp.fr/fdsnws/event/1/query?minlatitude=-13.543702&maxlatitude=-12.398181&minlongitude=44.708653&maxlongitude=46.417027&minmagnitude=0&format=text&nodata=404\n",
      "2025-05-09 18:27:16,448 - INFO - Échantillon des données reçues: #EventID|Time|Latitude|Longitude|Depth/km|Author|Catalog|Contributor|ContributorID|MagType|Magnitude|MagAuthor|EventLocationName\n",
      "revosima2025jasdob|2025-05-08T20:34:27.989999|-12.750200|45.561000|51.54|PhaseNet||REVOSIMA|revosima2025jasdob|ML|1.998769442|scmag@pitonparvedi|Northwest of Madagascar\n",
      "revosima2025jarxvf|2025-05-08T20:27:49.369999|-12.788200|45.619300|47.24|PhaseNet||REVOSIMA|revosima2025jarxvf|ML|2.374112553|scmag@pitonparvedi|Northwest of Madagascar\n",
      "revosima2025japnyj|2025-05-08T19:...\n",
      "2025-05-09 18:27:16,456 - INFO - Analyse des données IPGP\n",
      "2025-05-09 18:27:16,593 - INFO - DataFrame créé avec 5936 lignes et 13 colonnes\n",
      "2025-05-09 18:27:16,609 - INFO - Colonnes disponibles: ['EventID', 'Time', 'Latitude', 'Longitude', 'Depth/km', 'Author', 'Catalog', 'Contributor', 'ContributorID', 'MagType', 'Magnitude', 'MagAuthor', 'EventLocationName']\n",
      "2025-05-09 18:27:16,626 - INFO - Exemple de date originale: 2025-05-08T20:34:27.989999\n",
      "2025-05-09 18:27:20,000 - INFO - Exemple de date convertie: 08/05/2025 20:34\n",
      "2025-05-09 18:27:20,195 - INFO - Données IPGP transformées: 5936 lignes\n",
      "2025-05-09 18:27:20,234 - INFO - Données téléchargées et analysées: 5936 lignes\n",
      "2025-05-09 18:27:20,419 - INFO - Exemples de dates dans les nouvelles données: ['08/05/2025 20:34', '08/05/2025 20:27', '08/05/2025 19:16']\n",
      "2025-05-09 18:27:20,430 - INFO - Dates converties: ['2025-05-08 20:34:00', '2025-05-08 20:27:00', '2025-05-08 19:16:00']\n",
      "2025-05-09 18:27:20,470 - INFO - Données filtrées par date: 0 entrées plus récentes que 08/05/2025 20:34\n",
      "2025-05-09 18:27:20,476 - INFO - Aucune nouvelle donnée plus récente trouvée. Fin de la collecte.\n",
      "2025-05-09 18:27:20,509 - INFO - Collecte programmée tous les jours à 02:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Ce script télécharge régulièrement les nouvelles données sismiques depuis l'API IPGP\n",
    "et les ajoute à une base de données existante.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import schedule\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "import io\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"collecte_seismes_ipgp.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"CollecteSeismesIPGP\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - À MODIFIER SELON VOS BESOINS\n",
    "# =============================================================================\n",
    "\n",
    "# URL de l'API IPGP\n",
    "DATA_URL = \"https://ws.ipgp.fr/fdsnws/event/1/query?minlatitude=-13.543702&maxlatitude=-12.398181&minlongitude=44.708653&maxlongitude=46.417027&minmagnitude=0&format=text&nodata=404\"\n",
    "\n",
    "# Chemin vers votre fichier de base de données corrigé\n",
    "DATABASE_FILE = \"NewDataseisme_corrige.csv\"\n",
    "\n",
    "# Colonnes de votre base existante\n",
    "COLONNES_EXISTANTES = [\"Date\", \"Magnitude\", \"Latitude\", \"Longitude\", \"Profondeur\", \"origine\"]\n",
    "\n",
    "# Valeur par défaut pour la colonne \"origine\" dans les nouvelles données\n",
    "ORIGINE_DEFAULT = \"5\"  # 5 = IPGP (à adapter selon votre nomenclature)\n",
    "\n",
    "# Fréquence de collecte - par défaut, une fois par jour à 2h du matin\n",
    "HEURE_COLLECTE = \"02:00\"\n",
    "\n",
    "# =============================================================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# =============================================================================\n",
    "\n",
    "def extract_year(date_str):\n",
    "    \"\"\"\n",
    "    Extrait l'année d'une chaîne de date au format JJ/MM/AAAA ou JJ/MM/AA\n",
    "    \"\"\"\n",
    "    if not isinstance(date_str, str):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Si format ISO\n",
    "        if 'T' in date_str and '-' in date_str:\n",
    "            return int(date_str.split('-')[0])\n",
    "        \n",
    "        # Format standard JJ/MM/AAAA\n",
    "        parts = date_str.split('/')\n",
    "        if len(parts) < 3:\n",
    "            return None\n",
    "        \n",
    "        year_part = parts[2].split(' ')[0]\n",
    "        # Si année à 2 chiffres, ajouter \"20\" devant\n",
    "        if len(year_part) == 2:\n",
    "            year = int(\"20\" + year_part)\n",
    "        else:\n",
    "            year = int(year_part)\n",
    "        return year\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Erreur lors de l'extraction de l'année de '{date_str}': {e}\")\n",
    "        return None\n",
    "\n",
    "def normalize_date_format(date_str):\n",
    "    \"\"\"\n",
    "    Normalise le format de date vers JJ/MM/AAAA HH:MM\n",
    "    \"\"\"\n",
    "    if not isinstance(date_str, str):\n",
    "        return date_str\n",
    "    \n",
    "    try:\n",
    "        # Si la date est au format ISO\n",
    "        if 'T' in date_str and '-' in date_str:\n",
    "            # Gérer les cas avec ou sans secondes décimales et avec ou sans Z\n",
    "            date_str_cleaned = date_str.replace('Z', '')\n",
    "            if '.' in date_str_cleaned:\n",
    "                # Supprimer les millisecondes\n",
    "                date_parts = date_str_cleaned.split('.')\n",
    "                date_str_cleaned = date_parts[0]\n",
    "            \n",
    "            # Utiliser dateutil.parser pour une analyse plus robuste\n",
    "            try:\n",
    "                from dateutil import parser\n",
    "                dt = parser.parse(date_str_cleaned)\n",
    "            except ImportError:\n",
    "                # Fallback si dateutil n'est pas disponible\n",
    "                dt = datetime.fromisoformat(date_str_cleaned.replace('Z', '+00:00'))\n",
    "            \n",
    "            return dt.strftime('%d/%m/%Y %H:%M')\n",
    "        \n",
    "        # Format JJ/MM/AA(AA) HH:MM\n",
    "        parts = date_str.split('/')\n",
    "        if len(parts) < 3:\n",
    "            return date_str\n",
    "        \n",
    "        day = parts[0].strip()\n",
    "        month = parts[1].strip()\n",
    "        \n",
    "        # Traiter l'année et l'heure\n",
    "        year_time = parts[2].split(' ')\n",
    "        year = year_time[0].strip()\n",
    "        time = year_time[1].strip() if len(year_time) > 1 else \"00:00\"\n",
    "        \n",
    "        # Si l'année est à 2 chiffres, ajouter \"20\" devant\n",
    "        if len(year) == 2:\n",
    "            year = \"20\" + year\n",
    "        \n",
    "        # Reconstruire la date au format standard\n",
    "        return f\"{day.zfill(2)}/{month.zfill(2)}/{year} {time}\"\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Erreur lors de la normalisation de '{date_str}': {e}\")\n",
    "        return date_str\n",
    "\n",
    "def parse_datetime(date_str):\n",
    "    \"\"\"\n",
    "    Convertit différents formats de date en objet datetime\n",
    "    Gère à la fois les formats ISO et JJ/MM/AAAA\n",
    "    \"\"\"\n",
    "    if not isinstance(date_str, str):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Si c'est déjà un datetime\n",
    "        if isinstance(date_str, datetime):\n",
    "            return date_str\n",
    "        \n",
    "        # Si format ISO\n",
    "        if 'T' in date_str and '-' in date_str:\n",
    "            date_str_cleaned = date_str.replace('Z', '')\n",
    "            if '.' in date_str_cleaned:\n",
    "                # Supprimer les millisecondes si nécessaire\n",
    "                date_parts = date_str_cleaned.split('.')\n",
    "                date_str_cleaned = date_parts[0]\n",
    "            \n",
    "            try:\n",
    "                from dateutil import parser\n",
    "                return parser.parse(date_str_cleaned)\n",
    "            except ImportError:\n",
    "                return datetime.fromisoformat(date_str_cleaned)\n",
    "        \n",
    "        # Format JJ/MM/AAAA HH:MM\n",
    "        formats_to_try = [\n",
    "            '%d/%m/%Y %H:%M',\n",
    "            '%d/%m/%y %H:%M',\n",
    "            '%d/%m/%Y',\n",
    "            '%d/%m/%y'\n",
    "        ]\n",
    "        \n",
    "        for fmt in formats_to_try:\n",
    "            try:\n",
    "                return datetime.strptime(date_str, fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        # Si tous échouent, essayer un parser plus flexible\n",
    "        try:\n",
    "            from dateutil import parser\n",
    "            return parser.parse(date_str, dayfirst=True)\n",
    "        except (ImportError, ValueError):\n",
    "            pass\n",
    "        \n",
    "        logger.warning(f\"Impossible de parser la date: {date_str}\")\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors du parsing de la date '{date_str}': {e}\")\n",
    "        return None\n",
    "\n",
    "def create_unique_id(row):\n",
    "    \"\"\"\n",
    "    Crée un identifiant unique pour chaque entrée basé sur ses attributs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Formater les nombres avec une précision fixe\n",
    "        lat = f\"{float(str(row['Latitude']).replace(',', '.')):.6f}\"\n",
    "        lon = f\"{float(str(row['Longitude']).replace(',', '.')):.6f}\"\n",
    "        mag = f\"{float(str(row['Magnitude']).replace(',', '.')):.2f}\"\n",
    "        \n",
    "        # Construire l'identifiant unique\n",
    "        return f\"{row['Date']}_{lat}_{lon}_{mag}\"\n",
    "    except Exception as e:\n",
    "        # En cas d'erreur, utiliser les valeurs brutes\n",
    "        return f\"{row['Date']}_{row['Latitude']}_{row['Longitude']}_{row['Magnitude']}\"\n",
    "\n",
    "def parse_ipgp_data(text_data):\n",
    "    \"\"\"\n",
    "    Parse les données au format texte de l'API IPGP\n",
    "    Format attendu: format texte avec un entête commençant par #\n",
    "    Exemple:\n",
    "    #EventID | Time | Latitude | Longitude | Depth/km | Author | Catalog | Contributor | ContributorID | MagType | Magnitude | MagAuthor | EventLocationName\n",
    "    IPGP...  | 2025-05-01T14:30:00.0Z | -12.8123 | 45.3456 | 10.5 | ...\n",
    "    \"\"\"\n",
    "    logger.info(\"Analyse des données IPGP\")\n",
    "    \n",
    "    try:\n",
    "        # Extraire les lignes non commentées\n",
    "        data_lines = [line for line in text_data.strip().split('\\n') if not line.startswith('#')]\n",
    "        \n",
    "        # S'il n'y a pas de données, retourner un DataFrame vide\n",
    "        if not data_lines:\n",
    "            logger.warning(\"Aucune donnée trouvée dans la réponse IPGP\")\n",
    "            return pd.DataFrame(columns=COLONNES_EXISTANTES)\n",
    "        \n",
    "        # Obtenir les en-têtes (dernière ligne commentée)\n",
    "        header_lines = [line for line in text_data.strip().split('\\n') if line.startswith('#')]\n",
    "        if not header_lines:\n",
    "            logger.warning(\"Aucun en-tête trouvé dans les données IPGP, utilisation d'en-têtes par défaut\")\n",
    "            # En-têtes par défaut selon la documentation FDSN\n",
    "            headers = [\"EventID\", \"Time\", \"Latitude\", \"Longitude\", \"Depth/km\", \"Author\", \"Catalog\", \n",
    "                      \"Contributor\", \"ContributorID\", \"MagType\", \"Magnitude\", \"MagAuthor\", \"EventLocationName\"]\n",
    "        else:\n",
    "            # Utiliser le dernier en-tête trouvé\n",
    "            headers = [h.strip() for h in header_lines[-1].replace('#', '').split('|')]\n",
    "        \n",
    "        # Créer un DataFrame à partir des données\n",
    "        rows = []\n",
    "        for line in data_lines:\n",
    "            values = [v.strip() for v in line.split('|')]\n",
    "            # S'assurer que nous avons le bon nombre de valeurs\n",
    "            if len(values) == len(headers):\n",
    "                rows.append(dict(zip(headers, values)))\n",
    "            else:\n",
    "                logger.warning(f\"Ligne ignorée car nombre de colonnes incorrect: {line}\")\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        logger.info(f\"DataFrame créé avec {len(df)} lignes et {len(df.columns)} colonnes\")\n",
    "        logger.info(f\"Colonnes disponibles: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Mapper les colonnes IPGP vers notre format\n",
    "        df_mapped = pd.DataFrame()\n",
    "        \n",
    "        # Date (convertir de ISO à notre format)\n",
    "        if 'Time' in df.columns:\n",
    "            # Conserver la date originale pour le debugging\n",
    "            df['Original_Date'] = df['Time']\n",
    "            logger.info(f\"Exemple de date originale: {df['Time'].iloc[0] if len(df) > 0 else 'N/A'}\")\n",
    "            \n",
    "            # Convertir au format attendu\n",
    "            df_mapped['Date'] = df['Time'].apply(normalize_date_format)\n",
    "            logger.info(f\"Exemple de date convertie: {df_mapped['Date'].iloc[0] if len(df_mapped) > 0 else 'N/A'}\")\n",
    "        \n",
    "        # Magnitude\n",
    "        if 'Magnitude' in df.columns:\n",
    "            df_mapped['Magnitude'] = df['Magnitude'].astype(str).str.replace('.', ',')\n",
    "        \n",
    "        # Latitude\n",
    "        if 'Latitude' in df.columns:\n",
    "            df_mapped['Latitude'] = df['Latitude'].astype(str).str.replace('.', ',')\n",
    "        \n",
    "        # Longitude\n",
    "        if 'Longitude' in df.columns:\n",
    "            df_mapped['Longitude'] = df['Longitude'].astype(str).str.replace('.', ',')\n",
    "        \n",
    "        # Profondeur\n",
    "        if 'Depth/km' in df.columns:\n",
    "            df_mapped['Profondeur'] = df['Depth/km'].astype(str).str.replace('.', ',')\n",
    "        \n",
    "        # Origine (valeur par défaut pour IPGP)\n",
    "        df_mapped['origine'] = ORIGINE_DEFAULT\n",
    "        \n",
    "        # Vérifier que toutes les colonnes nécessaires sont présentes\n",
    "        missing_cols = set(COLONNES_EXISTANTES) - set(df_mapped.columns)\n",
    "        if missing_cols:\n",
    "            logger.warning(f\"Colonnes manquantes après le mapping: {missing_cols}\")\n",
    "            for col in missing_cols:\n",
    "                df_mapped[col] = np.nan\n",
    "        \n",
    "        logger.info(f\"Données IPGP transformées: {len(df_mapped)} lignes\")\n",
    "        return df_mapped[COLONNES_EXISTANTES]\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'analyse des données IPGP: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return pd.DataFrame(columns=COLONNES_EXISTANTES)\n",
    "\n",
    "# =============================================================================\n",
    "# FONCTION PRINCIPALE DE COLLECTE\n",
    "# =============================================================================\n",
    "\n",
    "def collecter_nouvelles_donnees():\n",
    "    \"\"\"\n",
    "    Fonction principale qui collecte les nouvelles données et les ajoute à la base existante\n",
    "    Ne collecte que les données plus récentes que la dernière date dans la base existante\n",
    "    \"\"\"\n",
    "    logger.info(\"Début de la collecte des nouvelles données sismiques\")\n",
    "    \n",
    "    try:\n",
    "        # 0. Vérifier si la base de données existante existe et obtenir la date la plus récente\n",
    "        date_derniere_entree = None\n",
    "        date_derniere_dt = None\n",
    "        \n",
    "        if os.path.exists(DATABASE_FILE):\n",
    "            try:\n",
    "                df_existing = pd.read_csv(DATABASE_FILE, sep=';', decimal=',')\n",
    "                logger.info(f\"Base de données existante chargée: {len(df_existing)} lignes\")\n",
    "                \n",
    "                # Afficher quelques exemples de dates pour le debugging\n",
    "                if 'Date' in df_existing.columns and len(df_existing) > 0:\n",
    "                    logger.info(f\"Exemples de dates dans la base existante: {df_existing['Date'].iloc[:3].tolist()}\")\n",
    "                    \n",
    "                    # Trouver la date la plus récente dans la base de données\n",
    "                    # Utiliser notre fonction robuste pour convertir les dates\n",
    "                    df_existing['Date_dt'] = df_existing['Date'].apply(parse_datetime)\n",
    "                    df_existing = df_existing.sort_values('Date_dt', ascending=False, na_position='last')\n",
    "                    \n",
    "                    # Ne garder que les lignes avec des dates valides\n",
    "                    df_existing = df_existing[df_existing['Date_dt'].notna()]\n",
    "                    \n",
    "                    if len(df_existing) > 0:\n",
    "                        date_derniere_entree = df_existing.iloc[0]['Date']\n",
    "                        date_derniere_dt = df_existing.iloc[0]['Date_dt']\n",
    "                        \n",
    "                        logger.info(f\"Date la plus récente dans la base: {date_derniere_entree} (parsed as {date_derniere_dt})\")\n",
    "                    else:\n",
    "                        logger.warning(\"Aucune date valide trouvée dans la base existante\")\n",
    "                    \n",
    "                    # Supprimer la colonne temporaire\n",
    "                    df_existing = df_existing.drop('Date_dt', axis=1)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erreur lors de la lecture de la base existante: {e}\")\n",
    "                import traceback\n",
    "                logger.error(traceback.format_exc())\n",
    "                date_derniere_entree = None\n",
    "        else:\n",
    "            logger.warning(f\"La base de données {DATABASE_FILE} n'existe pas encore\")\n",
    "        \n",
    "        # 1. Télécharger les nouvelles données depuis l'API IPGP\n",
    "        logger.info(f\"Téléchargement des données depuis {DATA_URL}\")\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(DATA_URL, headers=headers, timeout=60)\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Erreur lors du téléchargement des données: Code {response.status_code}\")\n",
    "            if response.status_code == 404 and response.text.strip() == \"404 Not Found: No data available\":\n",
    "                logger.info(\"Réponse normale de l'API: aucune donnée disponible pour les critères spécifiés\")\n",
    "            else:\n",
    "                logger.error(f\"Contenu de la réponse: {response.text[:500]}...\")\n",
    "            return\n",
    "        \n",
    "        # Afficher un échantillon de la réponse pour le debugging\n",
    "        logger.info(f\"Échantillon des données reçues: {response.text[:500]}...\")\n",
    "        \n",
    "        # 2. Analyser les données IPGP\n",
    "        df_new = parse_ipgp_data(response.text)\n",
    "        logger.info(f\"Données téléchargées et analysées: {len(df_new)} lignes\")\n",
    "        \n",
    "        if len(df_new) == 0:\n",
    "            logger.info(\"Aucune nouvelle donnée disponible. Fin de la collecte.\")\n",
    "            return\n",
    "        \n",
    "        # 3. Si la base de données n'existe pas, créer une nouvelle base avec toutes les données\n",
    "        if not os.path.exists(DATABASE_FILE) or not date_derniere_dt:\n",
    "            if not os.path.exists(DATABASE_FILE):\n",
    "                logger.warning(f\"La base de données {DATABASE_FILE} n'existe pas. Création d'une nouvelle base...\")\n",
    "            else:\n",
    "                logger.warning(\"Aucune date valide trouvée dans la base existante. Création d'une nouvelle base...\")\n",
    "            \n",
    "            df_new.to_csv(DATABASE_FILE, sep=';', index=False)\n",
    "            logger.info(f\"Base de données créée avec {len(df_new)} entrées\")\n",
    "            return\n",
    "        \n",
    "        # 4. Filtrer pour ne garder que les données plus récentes que la dernière date connue\n",
    "        # Convertir les dates du nouveau fichier avec notre fonction robuste\n",
    "        df_new['Date_dt'] = df_new['Date'].apply(parse_datetime)\n",
    "        \n",
    "        # Afficher des exemples pour le debugging\n",
    "        if len(df_new) > 0:\n",
    "            logger.info(f\"Exemples de dates dans les nouvelles données: {df_new['Date'].iloc[:3].tolist()}\")\n",
    "            logger.info(f\"Dates converties: {[str(dt) for dt in df_new['Date_dt'].iloc[:3].tolist()]}\")\n",
    "        \n",
    "        # Filtrer les entrées plus récentes que la dernière date connue\n",
    "        df_new_recent = df_new[df_new['Date_dt'] > date_derniere_dt].copy()\n",
    "        \n",
    "        logger.info(f\"Données filtrées par date: {len(df_new_recent)} entrées plus récentes que {date_derniere_entree}\")\n",
    "        \n",
    "        # Si aucune donnée plus récente, terminer\n",
    "        if len(df_new_recent) == 0:\n",
    "            logger.info(\"Aucune nouvelle donnée plus récente trouvée. Fin de la collecte.\")\n",
    "            return\n",
    "        \n",
    "        # Supprimer la colonne temporaire\n",
    "        df_new_recent = df_new_recent.drop('Date_dt', axis=1)\n",
    "        \n",
    "        # 5. Ajouter les nouvelles données à la base existante\n",
    "        df_combined = pd.concat([df_existing, df_new_recent], ignore_index=True)\n",
    "        \n",
    "        # 6. Trier par date\n",
    "        logger.info(\"Tri des données par date...\")\n",
    "        df_combined['Date_dt'] = df_combined['Date'].apply(parse_datetime)\n",
    "        df_combined = df_combined.sort_values('Date_dt', ascending=False, na_position='last')\n",
    "        df_combined = df_combined.drop('Date_dt', axis=1)\n",
    "        \n",
    "        # 7. Sauvegarder la base mise à jour\n",
    "        # Créer une sauvegarde de la base existante avant de la remplacer\n",
    "        backup_file = f\"{DATABASE_FILE}.bak\"\n",
    "        df_existing.to_csv(backup_file, sep=';', index=False)\n",
    "        logger.info(f\"Sauvegarde créée: {backup_file}\")\n",
    "        \n",
    "        # Sauvegarder la nouvelle version\n",
    "        df_combined.to_csv(DATABASE_FILE, sep=';', index=False)\n",
    "        logger.info(f\"Base de données mise à jour avec {len(df_new_recent)} nouvelles entrées\")\n",
    "        \n",
    "        # Afficher quelques statistiques\n",
    "        if 'Date' in df_combined.columns:\n",
    "            df_combined['Année'] = df_combined['Date'].apply(extract_year)\n",
    "            year_counts = df_combined['Année'].value_counts().sort_index()\n",
    "            logger.info(f\"Répartition des années dans la base mise à jour: \\n{year_counts}\")\n",
    "            df_combined = df_combined.drop('Année', axis=1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de la collecte des données: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "\n",
    "# =============================================================================\n",
    "# PROGRAMME PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Fonction principale du programme\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"DÉMARRAGE DU PROGRAMME DE COLLECTE DE DONNÉES SISMIQUES IPGP - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    logger.info(f\"Base de données: {DATABASE_FILE}\")\n",
    "    logger.info(f\"Source des données: {DATA_URL}\")\n",
    "    logger.info(f\"Colonnes désirées: {COLONNES_EXISTANTES}\")\n",
    "    logger.info(f\"Heure de collecte quotidienne: {HEURE_COLLECTE}\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    # Collecter les données immédiatement au lancement\n",
    "    collecter_nouvelles_donnees()\n",
    "    \n",
    "    # Programmer une collecte quotidienne\n",
    "    schedule.every().day.at(HEURE_COLLECTE).do(collecter_nouvelles_donnees)\n",
    "    logger.info(f\"Collecte programmée tous les jours à {HEURE_COLLECTE}\")\n",
    "    \n",
    "    # Boucle principale pour exécuter les tâches programmées\n",
    "    try:\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)  # Vérifier toutes les minutes\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"\\nProgramme arrêté par l'utilisateur\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur dans la boucle principale: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "\n",
    "# Point d'entrée du script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4b0a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (8.1.6)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu des données:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Profondeur</th>\n",
       "      <th>origine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08/05/2025 20:34</td>\n",
       "      <td>1,998769442</td>\n",
       "      <td>-12,750200</td>\n",
       "      <td>45,561000</td>\n",
       "      <td>51,54</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08/05/2025 20:27</td>\n",
       "      <td>2,374112553</td>\n",
       "      <td>-12,788200</td>\n",
       "      <td>45,619300</td>\n",
       "      <td>47,24</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/05/2025 19:16</td>\n",
       "      <td>1,667391108</td>\n",
       "      <td>-12,566000</td>\n",
       "      <td>45,175300</td>\n",
       "      <td>38,39</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/05/2025 01:04</td>\n",
       "      <td>2,025011775</td>\n",
       "      <td>-12,805300</td>\n",
       "      <td>45,580500</td>\n",
       "      <td>49,82</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07/05/2025 18:46</td>\n",
       "      <td>1,282582543</td>\n",
       "      <td>-12,805500</td>\n",
       "      <td>45,347200</td>\n",
       "      <td>43,46</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date    Magnitude    Latitude  Longitude Profondeur  origine\n",
       "0  08/05/2025 20:34  1,998769442  -12,750200  45,561000      51,54        5\n",
       "1  08/05/2025 20:27  2,374112553  -12,788200  45,619300      47,24        5\n",
       "2  08/05/2025 19:16  1,667391108  -12,566000  45,175300      38,39        5\n",
       "3  08/05/2025 01:04  2,025011775  -12,805300  45,580500      49,82        5\n",
       "4  07/05/2025 18:46  1,282582543  -12,805500  45,347200      43,46        5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de lignes: 14216\n",
      "Colonne de date identifiée: date\n",
      "\n",
      "Conversion des dates: 14216 succès, 0 échecs\n",
      "\n",
      "Vérification des années disponibles:\n",
      "[np.int32(2018), np.int32(2019), np.int32(2020), np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024), np.int32(2025)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684ef28add754cfcaeaff827cc5e38c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n    \\n<style>\\n.widget-label {\\n    font-size: 1.1em;\\n    font-weight: bold;\\n  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######### Tableau de bord des séismes avec esthétique améliorée #########\n",
    "\n",
    "%pip install pandas matplotlib ipywidgets python-dateutil seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "# Configuration pour un style plus esthétique\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "# Couleurs personnalisées\n",
    "COLORS = {\n",
    "    'primary': '#3498db',   # Bleu\n",
    "    'secondary': '#e74c3c',  # Rouge\n",
    "    'accent': '#2ecc71',    # Vert\n",
    "    'neutral': '#95a5a6',   # Gris\n",
    "    'dark': '#2c3e50',      # Bleu foncé\n",
    "    'light': '#ecf0f1'      # Blanc cassé\n",
    "}\n",
    "\n",
    "# CSS pour les widgets et le tableau de bord\n",
    "CSS = \"\"\"\n",
    "<style>\n",
    ".widget-label {\n",
    "    font-size: 1.1em;\n",
    "    font-weight: bold;\n",
    "    color: #2c3e50;\n",
    "}\n",
    ".section-title {\n",
    "    background-color: #3498db;\n",
    "    color: white;\n",
    "    padding: 10px 15px;\n",
    "    border-radius: 5px;\n",
    "    margin-top: 20px;\n",
    "    margin-bottom: 15px;\n",
    "    font-weight: bold;\n",
    "}\n",
    ".dashboard-title {\n",
    "    background-color: #2c3e50;\n",
    "    color: white;\n",
    "    padding: 15px;\n",
    "    border-radius: 8px;\n",
    "    margin-bottom: 25px;\n",
    "    text-align: center;\n",
    "    font-size: 1.4em;\n",
    "}\n",
    ".results-container {\n",
    "    background-color: #ecf0f1;\n",
    "    padding: 15px;\n",
    "    border-radius: 8px;\n",
    "    margin-top: 10px;\n",
    "    margin-bottom: 20px;\n",
    "    border-left: 5px solid #3498db;\n",
    "}\n",
    ".footer {\n",
    "    font-size: 0.8em;\n",
    "    color: #7f8c8d;\n",
    "    text-align: center;\n",
    "    margin-top: 30px;\n",
    "    font-style: italic;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# Configuration pour afficher plus de colonnes/lignes si nécessaire\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# Fonction pour traiter les formats de date multiples\n",
    "def parse_date_flexible(date_str):\n",
    "    \"\"\"\n",
    "    Fonction robuste pour parser différents formats de date\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Si c'est déjà un datetime\n",
    "        if isinstance(date_str, datetime):\n",
    "            return date_str\n",
    "            \n",
    "        # Nettoyer la chaîne si nécessaire\n",
    "        date_str = str(date_str).strip()\n",
    "        \n",
    "        # Essayer avec plusieurs formats\n",
    "        formats_to_try = [\n",
    "            '%d/%m/%Y %H:%M:%S',\n",
    "            '%d/%m/%Y %H:%M',\n",
    "            '%Y-%m-%dT%H:%M:%S.%f',  # Format ISO avec millisecondes\n",
    "            '%Y-%m-%dT%H:%M:%S',     # Format ISO sans millisecondes\n",
    "            '%d/%m/%y %H:%M',\n",
    "            '%Y-%m-%d %H:%M:%S'\n",
    "        ]\n",
    "        \n",
    "        for fmt in formats_to_try:\n",
    "            try:\n",
    "                return datetime.strptime(date_str, fmt)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        # Essayer d'utiliser dateutil.parser en dernier recours\n",
    "        try:\n",
    "            # Pour les formats JJ/MM/YYYY, indiquer dayfirst=True\n",
    "            if '/' in date_str and len(date_str.split('/')[0]) <= 2:\n",
    "                return parser.parse(date_str, dayfirst=True)\n",
    "            else:\n",
    "                return parser.parse(date_str)\n",
    "        except:\n",
    "            print(f\"Format de date non reconnu: {date_str}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du parsing de la date '{date_str}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Charger les données\n",
    "def charger_donnees():\n",
    "    # Essayer d'abord avec le séparateur point-virgule\n",
    "    try:\n",
    "        df = pd.read_csv('NewDataseisme_corrige.csv', sep=';')\n",
    "    except:\n",
    "        # Si ça échoue, essayer avec la virgule\n",
    "        try:\n",
    "            df = pd.read_csv('NewDataseisme_corrige.csv', sep=',')\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture du fichier: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Afficher un aperçu\n",
    "    print(\"Aperçu des données:\")\n",
    "    display(df.head())\n",
    "    print(f\"Nombre total de lignes: {len(df)}\")\n",
    "    \n",
    "    # Standardiser les noms de colonnes (normaliser la casse)\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    \n",
    "    # Trouver la colonne de date\n",
    "    date_column = None\n",
    "    for col in df.columns:\n",
    "        if 'date' in col.lower() or 'time' in col.lower() or 'heure' in col.lower():\n",
    "            date_column = col\n",
    "            break\n",
    "    \n",
    "    if not date_column:\n",
    "        print(\"Aucune colonne de date trouvée!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Colonne de date identifiée: {date_column}\")\n",
    "    \n",
    "    # Convertir la colonne de date\n",
    "    try:\n",
    "        # Créer une nouvelle colonne datetime\n",
    "        df['datetime'] = df[date_column].apply(parse_date_flexible)\n",
    "        \n",
    "        # Vérifier si la conversion a réussi\n",
    "        successful_conversions = df['datetime'].notna().sum()\n",
    "        failed_conversions = df['datetime'].isna().sum()\n",
    "        \n",
    "        print(f\"\\nConversion des dates: {successful_conversions} succès, {failed_conversions} échecs\")\n",
    "        \n",
    "        if failed_conversions > 0:\n",
    "            print(f\"Suppression de {failed_conversions} lignes avec des dates invalides\")\n",
    "            df = df.dropna(subset=['datetime'])\n",
    "        \n",
    "        # Utiliser la nouvelle colonne datetime pour l'analyse\n",
    "        date_column = 'datetime'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la conversion des dates: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "    # Extraire les composants temporels\n",
    "    df['annee'] = df[date_column].dt.year\n",
    "    df['mois'] = df[date_column].dt.month\n",
    "    df['jour'] = df[date_column].dt.day\n",
    "    \n",
    "    print(\"\\nVérification des années disponibles:\")\n",
    "    print(sorted(df['annee'].unique()))\n",
    "    \n",
    "    return df, date_column\n",
    "\n",
    "# Charger les données\n",
    "result = charger_donnees()\n",
    "if result is None:\n",
    "    raise ValueError(\"Impossible de continuer sans données valides!\")\n",
    "else:\n",
    "    df, date_column = result\n",
    "\n",
    "# Créer des widgets pour les filtres temporels\n",
    "annees_disponibles = sorted(df['annee'].unique())\n",
    "\n",
    "# Widget pour sélectionner l'année\n",
    "annee_dropdown = widgets.Dropdown(\n",
    "    options=annees_disponibles,\n",
    "    value=annees_disponibles[-1] if annees_disponibles else None,  # Sélectionner l'année la plus récente par défaut\n",
    "    description='Année:',\n",
    "    layout=widgets.Layout(width='200px'),\n",
    ")\n",
    "\n",
    "# Widget pour sélectionner le mois\n",
    "mois_noms = [\"Tous\", \"Janvier\", \"Février\", \"Mars\", \"Avril\", \"Mai\", \"Juin\", \n",
    "            \"Juillet\", \"Août\", \"Septembre\", \"Octobre\", \"Novembre\", \"Décembre\"]\n",
    "mois_options = [(mois, i) for i, mois in enumerate(mois_noms)]\n",
    "mois_dropdown = widgets.Dropdown(\n",
    "    options=mois_options,\n",
    "    value=0,  # 0 = tous les mois\n",
    "    description='Mois:',\n",
    "    layout=widgets.Layout(width='200px'),\n",
    ")\n",
    "\n",
    "# Zone d'affichage des résultats par période\n",
    "output_periode = widgets.Output()\n",
    "\n",
    "# Zone d'affichage du graphique annuel\n",
    "output_graphique_annuel = widgets.Output()\n",
    "\n",
    "# Fonction pour mettre à jour les résultats\n",
    "def update_resultats(change):\n",
    "    with output_periode:\n",
    "        clear_output()\n",
    "        \n",
    "        # Récupérer les valeurs des filtres\n",
    "        annee_selectionnee = annee_dropdown.value\n",
    "        mois_selectionne = mois_dropdown.value\n",
    "        \n",
    "        # Créer les masques de filtrage\n",
    "        mask_annee = df['annee'] == annee_selectionnee\n",
    "        \n",
    "        # Appliquer le filtre d'année\n",
    "        df_filtered = df[mask_annee].copy()\n",
    "        \n",
    "        # Appliquer le filtre de mois si nécessaire\n",
    "        if mois_selectionne > 0:\n",
    "            df_filtered = df_filtered[df_filtered['mois'] == mois_selectionne]\n",
    "        \n",
    "        # Afficher les résultats\n",
    "        nb_seismes = len(df_filtered)\n",
    "        \n",
    "        if nb_seismes == 0:\n",
    "            display(HTML(\"<div style='color:#e74c3c; font-weight:bold; padding:10px; background-color:#fdecea; border-radius:5px;'>Aucun séisme ne correspond aux critères de filtrage.</div>\"))\n",
    "            return\n",
    "        \n",
    "        # Afficher le nombre de séismes et la période\n",
    "        info_html = f\"\"\"\n",
    "        <div style='background-color:#eafaf1; padding:15px; border-radius:8px; margin-bottom:20px; border-left:5px solid #2ecc71;'>\n",
    "            <span style='font-size:1.3em; font-weight:bold;'>{nb_seismes} séismes</span> correspondent aux critères sélectionnés<br>\n",
    "            <span style='color:#2c3e50;'>Période: du {df_filtered[date_column].min().strftime('%d/%m/%Y')} au {df_filtered[date_column].max().strftime('%d/%m/%Y')}</span>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(info_html))\n",
    "        \n",
    "        # Créer une visualisation du nombre de séismes\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Si on a filtré par année et que tous les mois sont sélectionnés\n",
    "        if mois_selectionne == 0:\n",
    "            # Agréger par mois\n",
    "            monthly_counts = df_filtered.groupby('mois').size()\n",
    "            \n",
    "            # Créer un index correspondant à tous les mois\n",
    "            all_months = pd.Series(range(1, 13))\n",
    "            monthly_counts = monthly_counts.reindex(all_months).fillna(0)\n",
    "            \n",
    "            # Noms des mois pour les étiquettes\n",
    "            month_names = ['Jan', 'Fév', 'Mar', 'Avr', 'Mai', 'Juin', 'Juil', 'Août', 'Sep', 'Oct', 'Nov', 'Déc']\n",
    "            \n",
    "            # Créer le graphique\n",
    "            bars = plt.bar(range(1, 13), monthly_counts.values, color=COLORS['primary'], alpha=0.8)\n",
    "            \n",
    "            # Ajouter des annotations sur les barres\n",
    "            for i, bar in enumerate(bars):\n",
    "                height = bar.get_height()\n",
    "                if height > 0:\n",
    "                    plt.text(\n",
    "                        bar.get_x() + bar.get_width()/2.,\n",
    "                        height + max(monthly_counts.values) * 0.02,\n",
    "                        f'{int(height)}',\n",
    "                        ha='center', va='bottom', fontsize=10\n",
    "                    )\n",
    "            \n",
    "            plt.title(f\"Nombre de séismes par mois en {annee_selectionnee}\", fontsize=14, pad=20)\n",
    "            plt.xlabel(\"Mois\", fontsize=12)\n",
    "            plt.ylabel(\"Nombre de séismes\", fontsize=12)\n",
    "            plt.xticks(range(1, 13), month_names)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.ylim(0, max(monthly_counts.values) * 1.15)  # Laisser de l'espace pour les annotations\n",
    "        \n",
    "        # Si on a filtré par année et mois\n",
    "        else:\n",
    "            # Agréger par jour du mois\n",
    "            daily_counts = df_filtered.groupby('jour').size()\n",
    "            \n",
    "            # Nombre de jours dans ce mois\n",
    "            month_lengths = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "            if annee_selectionnee % 4 == 0 and (annee_selectionnee % 100 != 0 or annee_selectionnee % 400 == 0):\n",
    "                month_lengths[2] = 29  # Février en année bissextile\n",
    "            \n",
    "            days_in_month = month_lengths[mois_selectionne]\n",
    "            \n",
    "            # Créer un index pour tous les jours du mois\n",
    "            all_days = pd.Series(range(1, days_in_month + 1))\n",
    "            daily_counts = daily_counts.reindex(all_days).fillna(0)\n",
    "            \n",
    "            # Créer le graphique\n",
    "            bars = plt.bar(range(1, days_in_month + 1), daily_counts.values, color=COLORS['secondary'], alpha=0.8)\n",
    "            \n",
    "            # Ajouter des annotations sur les barres\n",
    "            for i, bar in enumerate(bars):\n",
    "                height = bar.get_height()\n",
    "                if height > 0:\n",
    "                    plt.text(\n",
    "                        bar.get_x() + bar.get_width()/2.,\n",
    "                        height + max(daily_counts.values) * 0.02 if max(daily_counts.values) > 0 else 0.5,\n",
    "                        f'{int(height)}',\n",
    "                        ha='center', va='bottom', fontsize=10\n",
    "                    )\n",
    "            \n",
    "            plt.title(f\"Nombre de séismes par jour en {mois_noms[mois_selectionne]} {annee_selectionnee}\", fontsize=14, pad=20)\n",
    "            plt.xlabel(\"Jour du mois\", fontsize=12)\n",
    "            plt.ylabel(\"Nombre de séismes\", fontsize=12)\n",
    "            plt.xticks(range(1, days_in_month + 1, 2))\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.ylim(0, max(daily_counts.values) * 1.15 if max(daily_counts.values) > 0 else 1)  # Laisser de l'espace pour les annotations\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Ajouter une bordure et un fond au graphique\n",
    "        plt.gca().spines['top'].set_visible(True)\n",
    "        plt.gca().spines['right'].set_visible(True)\n",
    "        plt.gca().spines['bottom'].set_visible(True)\n",
    "        plt.gca().spines['left'].set_visible(True)\n",
    "        \n",
    "        plt.gca().set_facecolor('#f8f9fa')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Fonction pour afficher le graphique du nombre total de séismes par année\n",
    "def afficher_graphique_annuel():\n",
    "    with output_graphique_annuel:\n",
    "        clear_output()\n",
    "        \n",
    "        # Calculer le nombre de séismes par année\n",
    "        annual_counts = df.groupby('annee').size()\n",
    "        \n",
    "        # Créer le graphique\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Créer un dégradé de couleurs\n",
    "        colors = [COLORS['dark']] * len(annual_counts)\n",
    "        \n",
    "        # Créer les barres\n",
    "        bars = plt.bar(annual_counts.index, annual_counts.values, color=colors, alpha=0.8)\n",
    "        \n",
    "        # Ajouter des annotations sur les barres\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width()/2.,\n",
    "                height + max(annual_counts.values) * 0.02,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold'\n",
    "            )\n",
    "        \n",
    "        plt.title(\"Nombre total de séismes par année\", fontsize=16, pad=20)\n",
    "        plt.xlabel(\"Année\", fontsize=14)\n",
    "        plt.ylabel(\"Nombre de séismes\", fontsize=14)\n",
    "        plt.xticks(annual_counts.index, fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.ylim(0, max(annual_counts.values) * 1.15)  # Laisser de l'espace pour les annotations\n",
    "        \n",
    "        # Ajouter une ligne de tendance\n",
    "        z = np.polyfit(range(len(annual_counts.index)), annual_counts.values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(annual_counts.index, p(range(len(annual_counts.index))), \n",
    "                 linestyle='--', color=COLORS['accent'], linewidth=2, \n",
    "                 label=f\"Tendance: {z[0]:.1f} séismes/an\")\n",
    "        \n",
    "        plt.legend(loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Ajouter une bordure et un fond au graphique\n",
    "        plt.gca().spines['top'].set_visible(True)\n",
    "        plt.gca().spines['right'].set_visible(True)\n",
    "        plt.gca().spines['bottom'].set_visible(True)\n",
    "        plt.gca().spines['left'].set_visible(True)\n",
    "        \n",
    "        plt.gca().set_facecolor('#f8f9fa')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Afficher un tableau récapitulatif\n",
    "        summary_html = \"\"\"\n",
    "        <div style='background-color:#eef5fb; padding:15px; border-radius:8px; margin-top:20px; border-left:5px solid #3498db;'>\n",
    "            <h3 style='margin-top:0; color:#2c3e50;'>Récapitulatif par année</h3>\n",
    "            <table style='width:100%; border-collapse:collapse;'>\n",
    "                <tr style='background-color:#3498db; color:white;'>\n",
    "                    <th style='padding:8px; text-align:left; border:1px solid #ddd;'>Année</th>\n",
    "                    <th style='padding:8px; text-align:right; border:1px solid #ddd;'>Nombre de séismes</th>\n",
    "                    <th style='padding:8px; text-align:right; border:1px solid #ddd;'>Pourcentage</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        total_seismes = annual_counts.sum()\n",
    "        \n",
    "        for year, count in annual_counts.items():\n",
    "            percentage = count / total_seismes * 100\n",
    "            row_color = '#f8f9fa' if year % 2 == 0 else 'white'\n",
    "            summary_html += f\"\"\"\n",
    "                <tr style='background-color:{row_color};'>\n",
    "                    <td style='padding:8px; text-align:left; border:1px solid #ddd;'>{year}</td>\n",
    "                    <td style='padding:8px; text-align:right; border:1px solid #ddd;'>{count}</td>\n",
    "                    <td style='padding:8px; text-align:right; border:1px solid #ddd;'>{percentage:.1f}%</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        summary_html += f\"\"\"\n",
    "                <tr style='background-color:#eaf2f8; font-weight:bold;'>\n",
    "                    <td style='padding:8px; text-align:left; border:1px solid #ddd;'>Total</td>\n",
    "                    <td style='padding:8px; text-align:right; border:1px solid #ddd;'>{total_seismes}</td>\n",
    "                    <td style='padding:8px; text-align:right; border:1px solid #ddd;'>100.0%</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        display(HTML(summary_html))\n",
    "\n",
    "# Observer les changements des widgets\n",
    "annee_dropdown.observe(update_resultats, names='value')\n",
    "mois_dropdown.observe(update_resultats, names='value')\n",
    "\n",
    "# Créer la mise en page du tableau de bord\n",
    "dashboard_title = widgets.HTML(f\"\"\"\n",
    "    {CSS}\n",
    "    <div class=\"dashboard-title\">Analyse des séismes | {len(df)} séismes enregistrés de {min(annees_disponibles)} à {max(annees_disponibles)}</div>\n",
    "\"\"\")\n",
    "\n",
    "filtres_section = widgets.HTML('<div class=\"section-title\">Filtrage par période</div>')\n",
    "resultats_section = widgets.HTML('<div class=\"results-container\" id=\"resultats-container\"></div>')\n",
    "graphique_annuel_section = widgets.HTML('<div class=\"section-title\">Nombre total de séismes par année</div>')\n",
    "footer = widgets.HTML('<div class=\"footer\">Tableau de bord créé pour l\\'analyse des données sismiques</div>')\n",
    "\n",
    "tableau_bord = widgets.VBox([\n",
    "    dashboard_title,\n",
    "    filtres_section,\n",
    "    widgets.HBox([annee_dropdown, mois_dropdown], layout=widgets.Layout(justify_content='center')),\n",
    "    output_periode,\n",
    "    graphique_annuel_section,\n",
    "    output_graphique_annuel,\n",
    "    footer\n",
    "])\n",
    "\n",
    "# Afficher le tableau de bord\n",
    "display(tableau_bord)\n",
    "\n",
    "# Déclencher l'affichage initial des résultats filtrés\n",
    "update_resultats(None)\n",
    "\n",
    "# Afficher le graphique annuel\n",
    "afficher_graphique_annuel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52bc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (8.1.6)\n",
      "Requirement already satisfied: folium in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: branca>=0.6.0 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from folium) (0.8.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from folium) (3.1.5)\n",
      "Requirement already satisfied: requests in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from folium) (2.32.3)\n",
      "Requirement already satisfied: xyzservices in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from folium) (2025.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->folium) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->folium) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\iznam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->folium) (2025.1.31)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\iznam\\appdata\\roaming\\python\\python311\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu des données:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Profondeur</th>\n",
       "      <th>origine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08/05/2025 20:34</td>\n",
       "      <td>1,998769442</td>\n",
       "      <td>-12,750200</td>\n",
       "      <td>45,561000</td>\n",
       "      <td>51,54</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08/05/2025 20:27</td>\n",
       "      <td>2,374112553</td>\n",
       "      <td>-12,788200</td>\n",
       "      <td>45,619300</td>\n",
       "      <td>47,24</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/05/2025 19:16</td>\n",
       "      <td>1,667391108</td>\n",
       "      <td>-12,566000</td>\n",
       "      <td>45,175300</td>\n",
       "      <td>38,39</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/05/2025 01:04</td>\n",
       "      <td>2,025011775</td>\n",
       "      <td>-12,805300</td>\n",
       "      <td>45,580500</td>\n",
       "      <td>49,82</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07/05/2025 18:46</td>\n",
       "      <td>1,282582543</td>\n",
       "      <td>-12,805500</td>\n",
       "      <td>45,347200</td>\n",
       "      <td>43,46</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date    Magnitude    Latitude  Longitude Profondeur  origine\n",
       "0  08/05/2025 20:34  1,998769442  -12,750200  45,561000      51,54        5\n",
       "1  08/05/2025 20:27  2,374112553  -12,788200  45,619300      47,24        5\n",
       "2  08/05/2025 19:16  1,667391108  -12,566000  45,175300      38,39        5\n",
       "3  08/05/2025 01:04  2,025011775  -12,805300  45,580500      49,82        5\n",
       "4  07/05/2025 18:46  1,282582543  -12,805500  45,347200      43,46        5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemples de format de date dans les données:\n",
      "17/7/19 23:57\n",
      "2/11/19 0:16\n",
      "20/7/19 9:48\n",
      "22/4/20 0:34\n",
      "6/6/18 14:17\n",
      "6/12/18 1:17\n",
      "26/6/21 1:19\n",
      "16/4/20 20:57\n",
      "2/9/19 23:42\n",
      "30/7/22 4:10\n",
      "\n",
      "Conversion des dates avec parse_date_flexible...\n",
      "Conversion des dates: 14216 succès, 0 échecs\n",
      "Données chargées: 14216 enregistrements\n",
      "\n",
      "Résumé statistique des variables numériques:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Profondeur</th>\n",
       "      <th>origine</th>\n",
       "      <th>Date_dt</th>\n",
       "      <th>Annee</th>\n",
       "      <th>Mois</th>\n",
       "      <th>Jour</th>\n",
       "      <th>Heure</th>\n",
       "      <th>JourSemaine</th>\n",
       "      <th>Trimestre</th>\n",
       "      <th>Semestre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14216</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216.000000</td>\n",
       "      <td>14216.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2020-12-14 04:19:44.890264576</td>\n",
       "      <td>2.078579</td>\n",
       "      <td>-12.810718</td>\n",
       "      <td>45.403393</td>\n",
       "      <td>34.274064</td>\n",
       "      <td>2.197383</td>\n",
       "      <td>2020-12-14 04:19:44.890264576</td>\n",
       "      <td>2020.443374</td>\n",
       "      <td>6.642656</td>\n",
       "      <td>15.367614</td>\n",
       "      <td>13.367262</td>\n",
       "      <td>3.045442</td>\n",
       "      <td>2.542839</td>\n",
       "      <td>1.474958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2018-05-10 23:19:00</td>\n",
       "      <td>0.143478</td>\n",
       "      <td>-13.507734</td>\n",
       "      <td>44.653645</td>\n",
       "      <td>-1.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2018-05-10 23:19:00</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2019-08-21 23:43:45</td>\n",
       "      <td>1.477645</td>\n",
       "      <td>-12.835257</td>\n",
       "      <td>45.339040</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2019-08-21 23:43:45</td>\n",
       "      <td>2019.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020-05-15 09:16:30</td>\n",
       "      <td>1.925589</td>\n",
       "      <td>-12.813657</td>\n",
       "      <td>45.365162</td>\n",
       "      <td>34.307955</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2020-05-15 09:16:30</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2021-12-21 00:15:45</td>\n",
       "      <td>2.567956</td>\n",
       "      <td>-12.793029</td>\n",
       "      <td>45.429007</td>\n",
       "      <td>38.777344</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2021-12-21 00:15:45</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2025-05-08 20:34:00</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>-11.714700</td>\n",
       "      <td>46.404640</td>\n",
       "      <td>184.508484</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2025-05-08 20:34:00</td>\n",
       "      <td>2025.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.822759</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.107224</td>\n",
       "      <td>7.364282</td>\n",
       "      <td>1.246709</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.747848</td>\n",
       "      <td>3.273409</td>\n",
       "      <td>8.966734</td>\n",
       "      <td>8.549266</td>\n",
       "      <td>2.052985</td>\n",
       "      <td>1.075219</td>\n",
       "      <td>0.499390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Date     Magnitude      Latitude  \\\n",
       "count                          14216  14216.000000  14216.000000   \n",
       "mean   2020-12-14 04:19:44.890264576      2.078579    -12.810718   \n",
       "min              2018-05-10 23:19:00      0.143478    -13.507734   \n",
       "25%              2019-08-21 23:43:45      1.477645    -12.835257   \n",
       "50%              2020-05-15 09:16:30      1.925589    -12.813657   \n",
       "75%              2021-12-21 00:15:45      2.567956    -12.793029   \n",
       "max              2025-05-08 20:34:00      5.900000    -11.714700   \n",
       "std                              NaN      0.822759      0.044781   \n",
       "\n",
       "          Longitude    Profondeur       origine  \\\n",
       "count  14216.000000  14216.000000  14216.000000   \n",
       "mean      45.403393     34.274064      2.197383   \n",
       "min       44.653645     -1.960000      1.000000   \n",
       "25%       45.339040     30.000000      1.000000   \n",
       "50%       45.365162     34.307955      2.000000   \n",
       "75%       45.429007     38.777344      3.000000   \n",
       "max       46.404640    184.508484      5.000000   \n",
       "std        0.107224      7.364282      1.246709   \n",
       "\n",
       "                             Date_dt         Annee          Mois  \\\n",
       "count                          14216  14216.000000  14216.000000   \n",
       "mean   2020-12-14 04:19:44.890264576   2020.443374      6.642656   \n",
       "min              2018-05-10 23:19:00   2018.000000      1.000000   \n",
       "25%              2019-08-21 23:43:45   2019.000000      4.000000   \n",
       "50%              2020-05-15 09:16:30   2020.000000      7.000000   \n",
       "75%              2021-12-21 00:15:45   2021.000000      9.000000   \n",
       "max              2025-05-08 20:34:00   2025.000000     12.000000   \n",
       "std                              NaN      1.747848      3.273409   \n",
       "\n",
       "               Jour         Heure   JourSemaine     Trimestre      Semestre  \n",
       "count  14216.000000  14216.000000  14216.000000  14216.000000  14216.000000  \n",
       "mean      15.367614     13.367262      3.045442      2.542839      1.474958  \n",
       "min        1.000000      0.000000      0.000000      1.000000      1.000000  \n",
       "25%        7.000000      4.000000      1.000000      2.000000      1.000000  \n",
       "50%       15.000000     17.000000      3.000000      3.000000      1.000000  \n",
       "75%       23.000000     21.000000      5.000000      3.000000      2.000000  \n",
       "max       31.000000     23.000000      6.000000      4.000000      2.000000  \n",
       "std        8.966734      8.549266      2.052985      1.075219      0.499390  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valeurs manquantes par colonne:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date           0\n",
       "Magnitude      0\n",
       "Latitude       0\n",
       "Longitude      0\n",
       "Profondeur     0\n",
       "origine        0\n",
       "Date_orig      0\n",
       "Date_dt        0\n",
       "Annee          0\n",
       "Mois           0\n",
       "Jour           0\n",
       "Heure          0\n",
       "JourSemaine    0\n",
       "Trimestre      0\n",
       "Semestre       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Période couverte: de 2018-05-10 23:19:00 à 2025-05-08 20:34:00\n",
      "Durée: 2554 jours\n",
      "\n",
      "--- INTERFACE D'ANALYSE SPATIO-TEMPORELLE DES SEISMES ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fc7e7fdb2749c7a786d7936a6ecfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntRangeSlider(value=(2018, 2025), continuous_update=False, description='Années:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77e5d260651420da0cfb91a88016381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "######Analyse Spatio-Temporelle des seismes  #####\n",
    "\n",
    "%pip install pandas numpy matplotlib seaborn ipywidgets folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "\n",
    "# Configuration pour afficher plus de colonnes/lignes si nécessaire\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# Fonction pour charger et préparer les données\n",
    "\"\"\"\n",
    "Fonction de conversion de dates optimisée pour le tableau de bord des séismes.\n",
    "Cette fonction combine les approches des deux tableaux de bord et ajoute\n",
    "des améliorations pour gérer les formats spécifiques détectés dans vos données.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_date_flexible(date_str):\n",
    "    \"\"\"\n",
    "    Fonction robuste pour parser différents formats de date\n",
    "    Gère spécifiquement les formats identifiés dans vos données comme \n",
    "    \"08/05/2025 01:04\" et \"14/2/25 6:08\"\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Si c'est déjà un datetime\n",
    "        if isinstance(date_str, datetime):\n",
    "            return date_str\n",
    "            \n",
    "        # Nettoyer la chaîne si nécessaire\n",
    "        date_str = str(date_str).strip()\n",
    "        \n",
    "        # Formats détectés dans vos données\n",
    "        formats_to_try = [\n",
    "            '%d/%m/%Y %H:%M',    # Pour \"08/05/2025 01:04\" (jour/mois/année sur 4 chiffres)\n",
    "            '%d/%m/%y %H:%M',    # Pour \"08/05/25 01:04\" (jour/mois/année sur 2 chiffres)\n",
    "            '%d/%m/%Y %H:%M:%S',\n",
    "            '%d/%m/%y %H:%M:%S',\n",
    "            # Formats sans zéros de remplissage (pour \"14/2/25 6:08\")\n",
    "            '%d/%-m/%y %-H:%M',  # Linux/Mac\n",
    "            '%d/%m/%y %H:%M',    # Windows (essai alternatif)\n",
    "            # Autres formats possibles\n",
    "            '%Y-%m-%dT%H:%M:%S.%f',  # Format ISO avec millisecondes\n",
    "            '%Y-%m-%dT%H:%M:%S',     # Format ISO sans millisecondes\n",
    "            '%Y-%m-%d %H:%M:%S'\n",
    "        ]\n",
    "        \n",
    "        # Essayer tous les formats explicites d'abord\n",
    "        for fmt in formats_to_try:\n",
    "            try:\n",
    "                # Adaptation pour Windows qui ne supporte pas %-\n",
    "                if '%-' in fmt and '/' in date_str:\n",
    "                    # Extraire les parties de la date pour un traitement manuel\n",
    "                    parts = date_str.split()\n",
    "                    if len(parts) == 2:  # Format \"14/2/25 6:08\"\n",
    "                        date_part = parts[0]\n",
    "                        time_part = parts[1]\n",
    "                        \n",
    "                        # Découper les composants de la date\n",
    "                        day, month, year = date_part.split('/')\n",
    "                        hour, minute = time_part.split(':')\n",
    "                        \n",
    "                        # Convertir en nombres\n",
    "                        day = int(day)\n",
    "                        month = int(month)\n",
    "                        \n",
    "                        # Déterminer si l'année est sur 2 ou 4 chiffres\n",
    "                        if len(year) == 2:\n",
    "                            # Convertir année sur 2 chiffres (20xx)\n",
    "                            year = 2000 + int(year)\n",
    "                        else:\n",
    "                            year = int(year)\n",
    "                            \n",
    "                        hour = int(hour)\n",
    "                        minute = int(minute)\n",
    "                        \n",
    "                        # Créer l'objet datetime\n",
    "                        return datetime(year, month, day, hour, minute)\n",
    "                else:\n",
    "                    return datetime.strptime(date_str, fmt)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Si aucun format explicite ne fonctionne, essayer avec dateutil.parser\n",
    "        try:\n",
    "            # Pour les formats JJ/MM/YYYY ou JJ/MM/YY, utiliser dayfirst=True\n",
    "            if '/' in date_str and len(date_str.split('/')[0]) <= 2:\n",
    "                # Exemple: \"14/2/25 6:08\" ou \"08/05/2025 01:04\"\n",
    "                return parser.parse(date_str, dayfirst=True)\n",
    "            else:\n",
    "                return parser.parse(date_str)\n",
    "        except:\n",
    "            # Dernière tentative: extraire manuellement les parties\n",
    "            try:\n",
    "                if '/' in date_str and ' ' in date_str:\n",
    "                    # Format supposé: \"jour/mois/année heure:minute\"\n",
    "                    date_part, time_part = date_str.split(' ', 1)\n",
    "                    day, month, year = date_part.split('/')\n",
    "                    \n",
    "                    # Gérer le cas où les heures et minutes sont séparées par ':'\n",
    "                    if ':' in time_part:\n",
    "                        hour, minute = time_part.split(':', 1)\n",
    "                    else:\n",
    "                        hour, minute = time_part, 0\n",
    "                    \n",
    "                    # Convertir en nombres et gérer les années sur 2 chiffres\n",
    "                    day = int(day)\n",
    "                    month = int(month)\n",
    "                    year = int(year)\n",
    "                    if year < 100:  # Année sur 2 chiffres\n",
    "                        year = 2000 + year\n",
    "                    \n",
    "                    hour = int(hour)\n",
    "                    minute = int(minute)\n",
    "                    \n",
    "                    return datetime(year, month, day, hour, minute)\n",
    "            except Exception as e:\n",
    "                print(f\"Échec de l'extraction manuelle: {e}\")\n",
    "        \n",
    "        print(f\"Format de date non reconnu: {date_str}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du parsing de la date '{date_str}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction complète pour charger et préparer les données avec gestion robuste des dates\n",
    "def charger_donnees():\n",
    "    # Charger les données\n",
    "    try:\n",
    "        df = pd.read_csv('NewDataseisme_corrige.csv', sep=';')\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv('NewDataseisme_corrige.csv', sep=',')\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la lecture du fichier: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Afficher un aperçu\n",
    "    print(\"Aperçu des données:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Afficher quelques exemples de dates pour diagnostic\n",
    "    print(\"\\nExemples de format de date dans les données:\")\n",
    "    if 'Date' in df.columns:\n",
    "        # Prendre quelques échantillons aléatoires pour mieux voir la diversité des formats\n",
    "        date_samples = df['Date'].sample(min(10, len(df))).tolist()\n",
    "        for sample in date_samples:\n",
    "            print(sample)\n",
    "    \n",
    "    # Convertir la colonne de date avec notre fonction améliorée\n",
    "    if 'Date' in df.columns:\n",
    "        print(\"\\nConversion des dates avec parse_date_flexible...\")\n",
    "        \n",
    "        # Conserver la date originale\n",
    "        df['Date_orig'] = df['Date']\n",
    "        \n",
    "        # Créer une nouvelle colonne pour les dates converties\n",
    "        df['Date_dt'] = df['Date'].apply(parse_date_flexible)\n",
    "        \n",
    "        # Vérifier le succès de la conversion\n",
    "        success_count = df['Date_dt'].notna().sum()\n",
    "        fail_count = df['Date_dt'].isna().sum()\n",
    "        \n",
    "        print(f\"Conversion des dates: {success_count} succès, {fail_count} échecs\")\n",
    "        \n",
    "        # Si certaines dates n'ont pas pu être converties, afficher des exemples\n",
    "        if fail_count > 0:\n",
    "            print(f\"Exemples de dates qui n'ont pas pu être converties:\")\n",
    "            failed_examples = df[df['Date_dt'].isna()]['Date'].head(5).tolist()\n",
    "            for example in failed_examples:\n",
    "                print(f\"  - '{example}'\")\n",
    "        \n",
    "        # Corriger le format des nombres (virgule à point)\n",
    "        for col in ['Magnitude', 'Latitude', 'Longitude', 'Profondeur']:\n",
    "            if col in df.columns:\n",
    "                if df[col].dtype == 'object':\n",
    "                    df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "        \n",
    "        # Extraire les composantes temporelles depuis la colonne convertie\n",
    "        if success_count > 0:\n",
    "            df['Annee'] = df['Date_dt'].dt.year\n",
    "            df['Mois'] = df['Date_dt'].dt.month\n",
    "            df['Jour'] = df['Date_dt'].dt.day\n",
    "            df['Heure'] = df['Date_dt'].dt.hour\n",
    "            df['JourSemaine'] = df['Date_dt'].dt.dayofweek\n",
    "            \n",
    "            # Créer des périodes trimestrielles et semestrielles\n",
    "            df['Trimestre'] = df['Date_dt'].dt.quarter\n",
    "            df['Semestre'] = (df['Mois'] <= 6).astype(int) + 1\n",
    "            \n",
    "            # Remplacer la colonne Date par Date_dt pour compatibilité avec le reste du code\n",
    "            df['Date'] = df['Date_dt']\n",
    "        else:\n",
    "            print(\"ERREUR: Aucune date n'a pu être convertie\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"ERREUR: Colonne 'Date' non trouvée dans les données\")\n",
    "        return None\n",
    "    \n",
    "    # Nettoyer les colonnes (supprimer les caractères \\r si présents)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.replace('\\r', '')\n",
    "    \n",
    "    print(f\"Données chargées: {len(df)} enregistrements\")\n",
    "    return df\n",
    "\n",
    "# Chargement des données\n",
    "df = charger_donnees()\n",
    "\n",
    "# Résumé statistique\n",
    "print(\"\\nRésumé statistique des variables numériques:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Vérifier les valeurs manquantes\n",
    "print(\"\\nValeurs manquantes par colonne:\")\n",
    "display(df.isna().sum())\n",
    "\n",
    "# Information sur la période couverte\n",
    "print(f\"\\nPériode couverte: de {df['Date'].min()} à {df['Date'].max()}\")\n",
    "print(f\"Durée: {(df['Date'].max() - df['Date'].min()).days} jours\")\n",
    "\n",
    "# ====================== INTERFACE INTERACTIVE ======================\n",
    "\n",
    "# Créer les widgets pour les filtres\n",
    "filter_out = widgets.Output()\n",
    "\n",
    "# Filtres temporels\n",
    "annees = sorted(df['Annee'].unique())\n",
    "mois = list(range(1, 13))\n",
    "mois_noms = ['Janvier', 'Février', 'Mars', 'Avril', 'Mai', 'Juin', \n",
    "             'Juillet', 'Août', 'Septembre', 'Octobre', 'Novembre', 'Décembre']\n",
    "mois_dict = {i+1: nom for i, nom in enumerate(mois_noms)}\n",
    "\n",
    "# Filtres magnitude\n",
    "min_mag = float(df['Magnitude'].min())\n",
    "max_mag = float(df['Magnitude'].max())\n",
    "\n",
    "# Filtres profondeur\n",
    "min_prof = float(df['Profondeur'].min())\n",
    "max_prof = float(df['Profondeur'].max())\n",
    "\n",
    "# Création des widgets\n",
    "annee_slider = widgets.IntRangeSlider(\n",
    "    value=[annees[0], annees[-1]],\n",
    "    min=annees[0],\n",
    "    max=annees[-1],\n",
    "    step=1,\n",
    "    description='Années:',\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width='70%')\n",
    ")\n",
    "\n",
    "mois_checkbox = widgets.SelectMultiple(\n",
    "    options=[(mois_dict[m], m) for m in mois],\n",
    "    value=mois,\n",
    "    description='Mois:',\n",
    "    layout=widgets.Layout(width='50%', height='100px')\n",
    ")\n",
    "\n",
    "magnitude_slider = widgets.FloatRangeSlider(\n",
    "    value=[min_mag, max_mag],\n",
    "    min=min_mag,\n",
    "    max=max_mag,\n",
    "    step=0.1,\n",
    "    description='Magnitude:',\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width='70%')\n",
    ")\n",
    "\n",
    "profondeur_slider = widgets.FloatRangeSlider(\n",
    "    value=[min_prof, max_prof],\n",
    "    min=min_prof,\n",
    "    max=max_prof,\n",
    "    step=5,\n",
    "    description='Profondeur:',\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width='70%')\n",
    ")\n",
    "\n",
    "# Types d'analyse\n",
    "analyse_type = widgets.RadioButtons(\n",
    "    options=['Distribution temporelle', 'Carte des séismes', 'Analyse par magnitude', 'Corrélations'],\n",
    "    description='Type d\\'analyse:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "filtrer_button = widgets.Button(\n",
    "    description='Appliquer les filtres',\n",
    "    button_style='primary',\n",
    "    tooltip='Cliquez pour appliquer les filtres',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Fonction pour appliquer les filtres\n",
    "def filtrer_donnees(df, annee_range, mois_selected, magnitude_range, profondeur_range):\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    # Filtrer par année\n",
    "    df_filtered = df_filtered[(df_filtered['Annee'] >= annee_range[0]) & \n",
    "                              (df_filtered['Annee'] <= annee_range[1])]\n",
    "    \n",
    "    # Filtrer par mois\n",
    "    df_filtered = df_filtered[df_filtered['Mois'].isin(mois_selected)]\n",
    "    \n",
    "    # Filtrer par magnitude\n",
    "    df_filtered = df_filtered[(df_filtered['Magnitude'] >= magnitude_range[0]) & \n",
    "                              (df_filtered['Magnitude'] <= magnitude_range[1])]\n",
    "    \n",
    "    # Filtrer par profondeur\n",
    "    df_filtered = df_filtered[(df_filtered['Profondeur'] >= profondeur_range[0]) & \n",
    "                              (df_filtered['Profondeur'] <= profondeur_range[1])]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Fonction pour créer une carte des séismes\n",
    "def creer_carte_seismes(df_filtered):\n",
    "    \"\"\"\n",
    "    Crée une carte des séismes avec un marqueur pour le volcan Fani Maoré,\n",
    "    les îles de Mayotte, et des cercles de distance.\n",
    "    \n",
    "    Paramètres:\n",
    "    - df_filtered: DataFrame contenant les données sismiques filtrées\n",
    "    \"\"\"\n",
    "    # Coordonnées précises du volcan Fani Maoré\n",
    "    fanimaoré = {\n",
    "        'nom': 'Fani Maoré',\n",
    "        'lat': -12.80,  # 12° 48′ sud\n",
    "        'lon': 45.467   # 45° 28′ est\n",
    "    }\n",
    "    \n",
    "    # Coordonnées des principales îles de Mayotte\n",
    "    mayotte_iles = [\n",
    "        {'nom': 'Grande-Terre', 'lat': -12.7817, 'lon': 45.2269, 'type': 'île principale'},\n",
    "        {'nom': 'Petite-Terre', 'lat': -12.7892, 'lon': 45.2804, 'type': 'île principale'},\n",
    "        {'nom': 'Mtsamboro', 'lat': -12.6964, 'lon': 45.0845, 'type': 'îlot'},\n",
    "        {'nom': 'Mbouzi', 'lat': -12.8121, 'lon': 45.2338, 'type': 'îlot'},\n",
    "        {'nom': 'Bandrélé', 'lat': -12.9085, 'lon': 45.1932, 'type': 'ville'}\n",
    "    ]\n",
    "    \n",
    "    # Créer une carte centrée entre Mayotte et Fani Maoré\n",
    "    center_lat = (fanimaoré['lat'] + mayotte_iles[0]['lat']) / 2\n",
    "    center_lon = (fanimaoré['lon'] + mayotte_iles[0]['lon']) / 2\n",
    "    \n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=8,\n",
    "                  tiles='CartoDB positron')\n",
    "    \n",
    "    # Ajouter un titre à la carte\n",
    "    title_html = '''\n",
    "        <div style=\"position: fixed; \n",
    "                    top: 10px; left: 50px; width: 350px; height: 30px; \n",
    "                    background-color: rgba(255, 255, 255, 0.8);\n",
    "                    border-radius: 5px; padding: 10px; z-index: 900;\">\n",
    "            <h4 style=\"margin: 0; text-align: center; color: #2c3e50;\">\n",
    "                Séismes autour de Mayotte et Fani Maoré ({} événements)\n",
    "            </h4>\n",
    "        </div>\n",
    "    '''.format(len(df_filtered))\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    \n",
    "    # Créer différents groupes de calques pour les contrôles\n",
    "    heat_layer = folium.FeatureGroup(name=\"Densité des séismes\").add_to(m)\n",
    "    marker_cluster = MarkerCluster(name=\"Séismes individuels\").add_to(m)\n",
    "    volcan_group = folium.FeatureGroup(name=\"Fani Maoré\", show=True).add_to(m)\n",
    "    iles_group = folium.FeatureGroup(name=\"Îles de Mayotte\", show=True).add_to(m)\n",
    "    distance_group = folium.FeatureGroup(name=\"Cercles de distance\", show=True).add_to(m)\n",
    "    \n",
    "    # Préparer les données pour la heatmap\n",
    "    heat_data = [[row['Latitude'], row['Longitude'], row['Magnitude']] \n",
    "                for _, row in df_filtered.iterrows()]\n",
    "    \n",
    "    # Améliorer les couleurs de la heatmap\n",
    "    gradient_dict = {'0.4': '#4575b4', '0.65': '#91bfdb', '0.8': '#fee090', '1.0': '#d73027'}\n",
    "    \n",
    "    # Ajouter la heatmap\n",
    "    HeatMap(heat_data, radius=15, blur=10, gradient=gradient_dict).add_to(heat_layer)\n",
    "    \n",
    "    # Normaliser les magnitudes pour la coloration\n",
    "    norm = Normalize(vmin=df_filtered['Magnitude'].min(), vmax=df_filtered['Magnitude'].max())\n",
    "    import matplotlib as mpl\n",
    "    cmap = mpl.colormaps['plasma']  # Méthode mise à jour pour obtenir la palette de couleurs\n",
    "    \n",
    "    # Fonction haversine pour calculer les distances\n",
    "    from math import radians, cos, sin, asin, sqrt\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        # Convertir degrés en radians\n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "        # Formule haversine\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        r = 6371  # Rayon de la Terre en km\n",
    "        return c * r\n",
    "    \n",
    "    # Ajouter des marqueurs pour chaque séisme\n",
    "    sample_size = min(500, len(df_filtered))  # Limiter le nombre de marqueurs pour la performance\n",
    "    for _, row in df_filtered.sample(sample_size).iterrows():\n",
    "        color = '#%02x%02x%02x' % tuple(int(255 * x) for x in cmap(norm(row['Magnitude']))[:3])\n",
    "        \n",
    "        # Calculer les distances\n",
    "        distance_volcan = haversine(row['Latitude'], row['Longitude'], \n",
    "                                   fanimaoré['lat'], fanimaoré['lon'])\n",
    "        \n",
    "        # Calculer la distance par rapport à Grande-Terre (Mayotte)\n",
    "        distance_mayotte = haversine(row['Latitude'], row['Longitude'], \n",
    "                                    mayotte_iles[0]['lat'], mayotte_iles[0]['lon'])\n",
    "        \n",
    "        # Créer le texte pour le popup\n",
    "        popup_html = f\"\"\"\n",
    "        <div style=\"width: 200px; font-family: Arial; font-size: 12px;\">\n",
    "            <h4 style=\"margin: 0 0 5px 0; color: #2c3e50;\">Séisme</h4>\n",
    "            <hr style=\"margin: 2px 0; border-color: #eee;\">\n",
    "            <p><b>Date:</b> {row['Date_dt'].strftime('%d/%m/%Y %H:%M')}</p>\n",
    "            <p><b>Magnitude:</b> <span style=\"color:{color}; font-weight:bold;\">{row['Magnitude']:.2f}</span></p>\n",
    "            <p><b>Profondeur:</b> {row['Profondeur']:.2f} km</p>\n",
    "            <p><b>Coordonnées:</b> {row['Latitude']:.4f}, {row['Longitude']:.4f}</p>\n",
    "            <hr style=\"margin: 5px 0;\">\n",
    "            <p><b>Distance à Fani Maoré:</b> {distance_volcan:.1f} km</p>\n",
    "            <p><b>Distance à Mayotte:</b> {distance_mayotte:.1f} km</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[row['Latitude'], row['Longitude']],\n",
    "            radius=max(3, row['Magnitude'] * 1.5),\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            fill_opacity=0.7,\n",
    "            popup=folium.Popup(popup_html, max_width=300),\n",
    "        ).add_to(marker_cluster)\n",
    "    \n",
    "    # Ajouter un marqueur pour le volcan Fani Maoré\n",
    "    folium.Marker(\n",
    "        location=[fanimaoré['lat'], fanimaoré['lon']],\n",
    "        popup=f\"\"\"\n",
    "        <div style=\"width: 200px;\">\n",
    "            <h4 style=\"margin: 0 0 5px 0; color: #e74c3c;\">Volcan Fani Maoré</h4>\n",
    "            <hr style=\"margin: 2px 0;\">\n",
    "            <p><b>Coordonnées:</b> 12° 48′ sud, 45° 28′ est</p>\n",
    "            <p><b>Type:</b> Volcan sous-marin</p>\n",
    "            <p><b>Statut:</b> Actif</p>\n",
    "        </div>\n",
    "        \"\"\",\n",
    "        tooltip=\"Fani Maoré\",\n",
    "        icon=folium.Icon(color='red', icon='info-sign')  # Modifié pour utiliser une icône par défaut\n",
    "    ).add_to(volcan_group)\n",
    "    \n",
    "    # Ajouter les îles de Mayotte\n",
    "    for ile in mayotte_iles:\n",
    "        # Différentes icônes selon le type\n",
    "        if ile['type'] == 'île principale':\n",
    "            icon = folium.Icon(color='green', icon='info-sign')  # Icône par défaut\n",
    "        elif ile['type'] == 'îlot':\n",
    "            icon = folium.Icon(color='green', icon='circle')\n",
    "        else:\n",
    "            icon = folium.Icon(color='blue', icon='info-sign')\n",
    "        \n",
    "        folium.Marker(\n",
    "            location=[ile['lat'], ile['lon']],\n",
    "            popup=f\"\"\"\n",
    "            <div style=\"width: 200px;\">\n",
    "                <h4 style=\"margin: 0 0 5px 0; color: #2ecc71;\">{ile['nom']}</h4>\n",
    "                <hr style=\"margin: 2px 0;\">\n",
    "                <p><b>Type:</b> {ile['type'].capitalize()}</p>\n",
    "                <p><b>Coordonnées:</b> {abs(ile['lat']):.4f}° S, {ile['lon']:.4f}° E</p>\n",
    "                <p><b>Distance à Fani Maoré:</b> {haversine(ile['lat'], ile['lon'], fanimaoré['lat'], fanimaoré['lon']):.1f} km</p>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            tooltip=ile['nom'],\n",
    "            icon=icon\n",
    "        ).add_to(iles_group)\n",
    "    \n",
    "    # Dessiner le contour approximatif de Grande-Terre de Mayotte\n",
    "    grande_terre_coords = [\n",
    "        [-12.7355, 45.0767], [-12.6593, 45.1043], [-12.6593, 45.1456],\n",
    "        [-12.6824, 45.1894], [-12.7309, 45.2250], [-12.7863, 45.2471],\n",
    "        [-12.8486, 45.2250], [-12.9109, 45.1784], [-12.9363, 45.1264],\n",
    "        [-12.9132, 45.0629], [-12.8532, 45.0408], [-12.7863, 45.0408],\n",
    "        [-12.7355, 45.0767]\n",
    "    ]\n",
    "    \n",
    "    folium.Polygon(\n",
    "        locations=grande_terre_coords,\n",
    "        color='green',\n",
    "        fill=True,\n",
    "        fill_color='green',\n",
    "        fill_opacity=0.3,\n",
    "        tooltip=\"Grande-Terre (Mayotte)\"\n",
    "    ).add_to(iles_group)\n",
    "    \n",
    "    # Dessiner le contour approximatif de Petite-Terre\n",
    "    petite_terre_coords = [\n",
    "        [-12.7723, 45.2757], [-12.7769, 45.2840], [-12.7909, 45.2881],\n",
    "        [-12.8038, 45.2798], [-12.8015, 45.2715], [-12.7769, 45.2674],\n",
    "        [-12.7723, 45.2757]\n",
    "    ]\n",
    "    \n",
    "    folium.Polygon(\n",
    "        locations=petite_terre_coords,\n",
    "        color='green',\n",
    "        fill=True,\n",
    "        fill_color='green',\n",
    "        fill_opacity=0.3,\n",
    "        tooltip=\"Petite-Terre (Mayotte)\"\n",
    "    ).add_to(iles_group)\n",
    "    \n",
    "    # Ajouter des cercles de distance autour de Fani Maoré\n",
    "    distances = [5, 10, 20, 50]\n",
    "    colors = ['#ff0000', '#ff6600', '#ffcc00', '#ffff00']\n",
    "    \n",
    "    for i, distance in enumerate(distances):\n",
    "        folium.Circle(\n",
    "            location=[fanimaoré['lat'], fanimaoré['lon']],\n",
    "            radius=distance * 1000,  # Convertir km en mètres\n",
    "            color=colors[i],\n",
    "            fill=True,\n",
    "            fill_opacity=0.1,\n",
    "            weight=2,\n",
    "            popup=f\"Rayon de {distance} km autour de Fani Maoré\"\n",
    "        ).add_to(distance_group)\n",
    "    \n",
    "    # Créer aussi un cercle de distance autour de Grande-Terre pour référence\n",
    "    folium.Circle(\n",
    "        location=[mayotte_iles[0]['lat'], mayotte_iles[0]['lon']],\n",
    "        radius=20 * 1000,  # 20 km\n",
    "        color='green',\n",
    "        fill=True,\n",
    "        fill_opacity=0.1,\n",
    "        weight=2,\n",
    "        popup=\"Rayon de 20 km autour de Mayotte (Grande-Terre)\"\n",
    "    ).add_to(distance_group)\n",
    "    \n",
    "    # Ajouter une légende claire\n",
    "    legend_html = '''\n",
    "         <div style=\"position: fixed; \n",
    "                     bottom: 50px; right: 50px; width: 200px; height: auto;\n",
    "                     background-color: white; border-radius: 5px;\n",
    "                     box-shadow: 0 0 15px rgba(0,0,0,0.2);\n",
    "                     padding: 10px; z-index: 900;\">\n",
    "             <p style=\"margin:0; text-align:center; font-weight:bold;\">Magnitude des séismes</p>\n",
    "             <hr style=\"margin: 5px 0;\">\n",
    "             <div style=\"display: flex; justify-content: space-between;\">\n",
    "                 <span>Faible</span>\n",
    "                 <div style=\"width: 100px; height: 20px; background: linear-gradient(to right, #440154, #482878, #3e4989, #31688e, #26828e, #1f9e89, #35b779, #6ece58, #b5de2b, #fde725);\"></div>\n",
    "                 <span>Forte</span>\n",
    "             </div>\n",
    "             <div style=\"display: flex; justify-content: space-between; margin-top: 5px;\">\n",
    "                 <span>''' + f\"{df_filtered['Magnitude'].min():.1f}\" + '''</span>\n",
    "                 <span>''' + f\"{df_filtered['Magnitude'].max():.1f}\" + '''</span>\n",
    "             </div>\n",
    "             <hr style=\"margin: 10px 0;\">\n",
    "             <p style=\"margin:0; text-align:center; font-weight:bold;\">Points d'intérêt</p>\n",
    "             <div style=\"margin-top: 5px;\">\n",
    "                 <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "                     <div style=\"background-color: red; width: 15px; height: 15px; border-radius: 50%; margin-right: 5px;\"></div>\n",
    "                     <span>Fani Maoré (volcan)</span>\n",
    "                 </div>\n",
    "                 <div style=\"display: flex; align-items: center; margin-bottom: 5px;\">\n",
    "                     <div style=\"background-color: green; width: 15px; height: 15px; border-radius: 50%; margin-right: 5px;\"></div>\n",
    "                     <span>Îles principales</span>\n",
    "                 </div>\n",
    "                 <div style=\"display: flex; align-items: center;\">\n",
    "                     <div style=\"background-color: blue; width: 15px; height: 15px; border-radius: 50%; margin-right: 5px;\"></div>\n",
    "                     <span>Villes</span>\n",
    "                 </div>\n",
    "             </div>\n",
    "             <hr style=\"margin: 10px 0;\">\n",
    "             <p style=\"margin:0; text-align:center; font-weight:bold;\">Distances</p>\n",
    "             <div style=\"margin-top: 5px;\">\n",
    "                 <div style=\"display: flex; align-items: center; margin-bottom: 3px;\">\n",
    "                     <div style=\"width: 15px; height: 15px; background-color: #ff0000; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "                     <span>5 km (Fani Maoré)</span>\n",
    "                 </div>\n",
    "                 <div style=\"display: flex; align-items: center; margin-bottom: 3px;\">\n",
    "                     <div style=\"width: 15px; height: 15px; background-color: #ff6600; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "                     <span>10 km (Fani Maoré)</span>\n",
    "                 </div>\n",
    "                 <div style=\"display: flex; align-items: center; margin-bottom: 3px;\">\n",
    "                     <div style=\"width: 15px; height: 15px; background-color: #ffcc00; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "                     <span>20 km (Fani Maoré)</span>\n",
    "                 </div>\n",
    "                 <div style=\"display: flex; align-items: center; margin-bottom: 3px;\">\n",
    "                     <div style=\"width: 15px; height: 15px; background-color: #ffff00; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "                     <span>50 km (Fani Maoré)</span>\n",
    "                 </div>\n",
    "                 <div style=\"display: flex; align-items: center;\">\n",
    "                     <div style=\"width: 15px; height: 15px; background-color: green; margin-right: 5px; border-radius: 50%;\"></div>\n",
    "                     <span>20 km (Mayotte)</span>\n",
    "                 </div>\n",
    "             </div>\n",
    "         </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    # Ajouter un contrôle de calques\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "# Analyse de distribution temporelle\n",
    "def analyse_temporelle(df_filtered, periode='Mois'):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    if periode == 'Annee':\n",
    "        counts = df_filtered['Annee'].value_counts().sort_index()\n",
    "        plt.bar(counts.index, counts.values)\n",
    "        plt.xlabel('Année')\n",
    "        plt.title(f'Distribution des séismes par année ({len(df_filtered)} séismes)')\n",
    "        plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "        \n",
    "    elif periode == 'Mois':\n",
    "        counts = df_filtered['Mois'].value_counts().sort_index()\n",
    "        plt.bar(counts.index, counts.values)\n",
    "        plt.xlabel('Mois')\n",
    "        plt.xticks(range(1, 13), mois_noms, rotation=45)\n",
    "        plt.title(f'Distribution des séismes par mois ({len(df_filtered)} séismes)')\n",
    "        \n",
    "    elif periode == 'Jour':\n",
    "        counts = df_filtered['Jour'].value_counts().sort_index()\n",
    "        plt.bar(counts.index, counts.values)\n",
    "        plt.xlabel('Jour du mois')\n",
    "        plt.title(f'Distribution des séismes par jour du mois ({len(df_filtered)} séismes)')\n",
    "        \n",
    "    elif periode == 'Heure':\n",
    "        counts = df_filtered['Heure'].value_counts().sort_index()\n",
    "        plt.bar(counts.index, counts.values)\n",
    "        plt.xlabel('Heure de la journée')\n",
    "        plt.xticks(range(0, 24))\n",
    "        plt.title(f'Distribution des séismes par heure ({len(df_filtered)} séismes)')\n",
    "        \n",
    "    elif periode == 'JourSemaine':\n",
    "        jours = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']\n",
    "        counts = df_filtered['JourSemaine'].value_counts().sort_index()\n",
    "        plt.bar(counts.index, counts.values)\n",
    "        plt.xlabel('Jour de la semaine')\n",
    "        plt.xticks(range(7), jours, rotation=45)\n",
    "        plt.title(f'Distribution des séismes par jour de la semaine ({len(df_filtered)} séismes)')\n",
    "    \n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher les statistiques\n",
    "    print(f\"Distribution temporelle par {periode.lower()}:\")\n",
    "    if periode == 'Mois':\n",
    "        for idx, count in counts.items():\n",
    "            print(f\"{mois_dict[idx]}: {count} séismes ({count/len(df_filtered)*100:.1f}%)\")\n",
    "    elif periode == 'JourSemaine':\n",
    "        jours = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']\n",
    "        for idx, count in counts.items():\n",
    "            print(f\"{jours[idx]}: {count} séismes ({count/len(df_filtered)*100:.1f}%)\")\n",
    "    else:\n",
    "        for idx, count in counts.items():\n",
    "            print(f\"{idx}: {count} séismes ({count/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "# Analyse par magnitude\n",
    "def analyse_magnitude(df_filtered):\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Subplot 1: Distribution des magnitudes\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.histplot(df_filtered['Magnitude'], bins=30, kde=True)\n",
    "    plt.title('Distribution des magnitudes')\n",
    "    plt.xlabel('Magnitude')\n",
    "    plt.ylabel('Fréquence')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Magnitude moyenne par année\n",
    "    plt.subplot(2, 2, 2)\n",
    "    mag_by_year = df_filtered.groupby('Annee')['Magnitude'].mean()\n",
    "    plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    plt.bar(mag_by_year.index, mag_by_year.values)\n",
    "    plt.title('Magnitude moyenne par année')\n",
    "    plt.xlabel('Année')\n",
    "    plt.ylabel('Magnitude moyenne')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Magnitude vs Profondeur\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(df_filtered['Profondeur'], df_filtered['Magnitude'], alpha=0.5)\n",
    "    plt.title('Magnitude vs Profondeur')\n",
    "    plt.xlabel('Profondeur (km)')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: Évolution temporelle des magnitudes\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(df_filtered['Date'], df_filtered['Magnitude'], alpha=0.5, s=10)\n",
    "    plt.title('Évolution temporelle des magnitudes')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques sur les magnitudes\n",
    "    print(\"Statistiques des magnitudes:\")\n",
    "    print(f\"Moyenne: {df_filtered['Magnitude'].mean():.2f}\")\n",
    "    print(f\"Médiane: {df_filtered['Magnitude'].median():.2f}\")\n",
    "    print(f\"Min: {df_filtered['Magnitude'].min():.2f}\")\n",
    "    print(f\"Max: {df_filtered['Magnitude'].max():.2f}\")\n",
    "    print(f\"Écart-type: {df_filtered['Magnitude'].std():.2f}\")\n",
    "    \n",
    "    # Répartition par catégorie de magnitude\n",
    "    bins = [0, 1, 2, 3, 4, 5, float('inf')]\n",
    "    labels = ['0-1', '1-2', '2-3', '3-4', '4-5', '5+']\n",
    "    df_filtered['MagnitudeCategorie'] = pd.cut(df_filtered['Magnitude'], bins=bins, labels=labels)\n",
    "    cat_counts = df_filtered['MagnitudeCategorie'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"\\nRépartition par catégorie de magnitude:\")\n",
    "    for category, count in cat_counts.items():\n",
    "        print(f\"Magnitude {category}: {count} séismes ({count/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "# Analyse des corrélations\n",
    "def analyse_correlations(df_filtered):\n",
    "    # Sélectionner les colonnes numériques pertinentes\n",
    "    cols_numeriques = ['Magnitude', 'Profondeur', 'Latitude', 'Longitude', 'Annee', 'Mois', 'Jour', 'Heure']\n",
    "    df_num = df_filtered[cols_numeriques]\n",
    "    \n",
    "    # Calculer la matrice de corrélation\n",
    "    corr_matrix = df_num.corr()\n",
    "    \n",
    "    # Afficher la matrice de corrélation\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Matrice de corrélation')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identifier les corrélations les plus fortes (en valeur absolue)\n",
    "    corr_unstack = corr_matrix.unstack()\n",
    "    corr_unstack = corr_unstack[corr_unstack < 1.0]  # Supprimer les auto-corrélations (= 1.0)\n",
    "    corr_abs = corr_unstack.abs()\n",
    "    corr_sorted = corr_abs.sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Corrélations les plus fortes (en valeur absolue):\")\n",
    "    # Convertir items() en liste et prendre les 10 premiers éléments\n",
    "    top_correlations = list(corr_sorted.items())[:10]\n",
    "    for i, ((col1, col2), corr_value) in enumerate(top_correlations):\n",
    "        print(f\"{col1} - {col2}: {corr_matrix.loc[col1, col2]:.3f}\")\n",
    "\n",
    "# Fonction principale pour mettre à jour l'analyse\n",
    "def update_analysis(b):\n",
    "    with filter_out:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Récupérer les valeurs des filtres\n",
    "        annee_range = annee_slider.value\n",
    "        mois_selected = mois_checkbox.value\n",
    "        magnitude_range = magnitude_slider.value\n",
    "        profondeur_range = profondeur_slider.value\n",
    "        \n",
    "        # Appliquer les filtres\n",
    "        df_filtered = filtrer_donnees(df, annee_range, mois_selected, magnitude_range, profondeur_range)\n",
    "        \n",
    "        print(f\"Données filtrées: {len(df_filtered)} séismes sur {len(df)} ({len(df_filtered)/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        if len(df_filtered) == 0:\n",
    "            print(\"Aucune donnée ne correspond aux critères de filtrage!\")\n",
    "            return\n",
    "        \n",
    "        # Effectuer l'analyse sélectionnée\n",
    "        analysis_type = analyse_type.value\n",
    "        \n",
    "        if analysis_type == 'Distribution temporelle':\n",
    "            # Créer des onglets pour différentes périodes temporelles\n",
    "            periode_tabs = widgets.Tab()\n",
    "            children = []\n",
    "            \n",
    "            for periode in ['Annee', 'Mois', 'Jour', 'Heure', 'JourSemaine']:\n",
    "                output = widgets.Output()\n",
    "                with output:\n",
    "                    analyse_temporelle(df_filtered, periode)\n",
    "                children.append(output)\n",
    "            \n",
    "            periode_tabs.children = children\n",
    "            periode_tabs.set_title(0, 'Années')\n",
    "            periode_tabs.set_title(1, 'Mois')\n",
    "            periode_tabs.set_title(2, 'Jours')\n",
    "            periode_tabs.set_title(3, 'Heures')\n",
    "            periode_tabs.set_title(4, 'Jours semaine')\n",
    "            \n",
    "            display(periode_tabs)\n",
    "            \n",
    "        elif analysis_type == 'Carte des séismes':\n",
    "            try:\n",
    "                m = creer_carte_seismes(df_filtered)\n",
    "                display(m)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la création de la carte: {e}\")\n",
    "                print(\"Note: Pour voir les cartes, assurez-vous d'avoir installé folium (pip install folium)\")\n",
    "                \n",
    "                # Alternative sans folium - carte simple avec Matplotlib\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.scatter(df_filtered['Longitude'], df_filtered['Latitude'], \n",
    "                            c=df_filtered['Magnitude'], cmap='plasma', alpha=0.6)\n",
    "                plt.colorbar(label='Magnitude')\n",
    "                plt.title('Carte des séismes (Alternative simple)')\n",
    "                plt.xlabel('Longitude')\n",
    "                plt.ylabel('Latitude')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.show()\n",
    "                \n",
    "        elif analysis_type == 'Analyse par magnitude':\n",
    "            analyse_magnitude(df_filtered)\n",
    "            \n",
    "        elif analysis_type == 'Corrélations':\n",
    "            analyse_correlations(df_filtered)\n",
    "\n",
    "# Connecter le bouton à la fonction d'actualisation\n",
    "filtrer_button.on_click(update_analysis)\n",
    "\n",
    "# Afficher l'interface\n",
    "print(\"\\n--- INTERFACE D'ANALYSE SPATIO-TEMPORELLE DES SEISMES ---\")\n",
    "controls = widgets.VBox([\n",
    "    widgets.HBox([annee_slider]),\n",
    "    widgets.HBox([mois_checkbox]),\n",
    "    widgets.HBox([magnitude_slider]),\n",
    "    widgets.HBox([profondeur_slider]),\n",
    "    widgets.HBox([analyse_type]),\n",
    "    widgets.HBox([filtrer_button])\n",
    "])\n",
    "\n",
    "display(controls)\n",
    "display(filter_out)\n",
    "\n",
    "# Lancer l'analyse avec les filtres par défaut\n",
    "update_analysis(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4468066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu des données:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Profondeur</th>\n",
       "      <th>origine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08/05/2025 20:34</td>\n",
       "      <td>1,998769442</td>\n",
       "      <td>-12,750200</td>\n",
       "      <td>45,561000</td>\n",
       "      <td>51,54</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08/05/2025 20:27</td>\n",
       "      <td>2,374112553</td>\n",
       "      <td>-12,788200</td>\n",
       "      <td>45,619300</td>\n",
       "      <td>47,24</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/05/2025 19:16</td>\n",
       "      <td>1,667391108</td>\n",
       "      <td>-12,566000</td>\n",
       "      <td>45,175300</td>\n",
       "      <td>38,39</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/05/2025 01:04</td>\n",
       "      <td>2,025011775</td>\n",
       "      <td>-12,805300</td>\n",
       "      <td>45,580500</td>\n",
       "      <td>49,82</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07/05/2025 18:46</td>\n",
       "      <td>1,282582543</td>\n",
       "      <td>-12,805500</td>\n",
       "      <td>45,347200</td>\n",
       "      <td>43,46</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date    Magnitude    Latitude  Longitude Profondeur  origine\n",
       "0  08/05/2025 20:34  1,998769442  -12,750200  45,561000      51,54        5\n",
       "1  08/05/2025 20:27  2,374112553  -12,788200  45,619300      47,24        5\n",
       "2  08/05/2025 19:16  1,667391108  -12,566000  45,175300      38,39        5\n",
       "3  08/05/2025 01:04  2,025011775  -12,805300  45,580500      49,82        5\n",
       "4  07/05/2025 18:46  1,282582543  -12,805500  45,347200      43,46        5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemples de format de date dans les données:\n",
      "08/05/2025 20:34\n",
      "08/05/2025 20:27\n",
      "08/05/2025 19:16\n",
      "08/05/2025 01:04\n",
      "07/05/2025 18:46\n",
      "\n",
      "Conversion des dates avec parse_date_flexible...\n",
      "Conversion des dates: 14216 succès, 0 échecs\n",
      "Données chargées: 14216 enregistrements\n",
      "Période couverte: 2018-05-10 à 2025-05-08\n",
      "\n",
      "--- DASHBOARD D'ANALYSE DES TENDANCES SISMIQUES ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944f25a2f41f4b6f965c67d4db013331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Filtres temporels', style=LabelStyle(font_weight='bold')),)), HBox(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d048b28a55684f7f8fff516f586615cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######### Analyses des tendances seismiques   ##############\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import calendar\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Charger les données\n",
    "# Fonction robuste pour analyser les dates avec plusieurs formats\n",
    "def parse_date_flexible(date_str):\n",
    "    \"\"\"\n",
    "    Fonction robuste pour parser différents formats de date\n",
    "    Gère spécifiquement les formats identifiés dans vos données comme \n",
    "    \"08/05/2025 01:04\" et \"14/2/25 6:08\"\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Si c'est déjà un datetime\n",
    "        if isinstance(date_str, datetime):\n",
    "            return date_str\n",
    "            \n",
    "        # Nettoyer la chaîne si nécessaire\n",
    "        date_str = str(date_str).strip()\n",
    "        \n",
    "        # Formats détectés dans vos données\n",
    "        formats_to_try = [\n",
    "            '%d/%m/%Y %H:%M',    # Pour \"08/05/2025 01:04\" (jour/mois/année sur 4 chiffres)\n",
    "            '%d/%m/%y %H:%M',    # Pour \"08/05/25 01:04\" (jour/mois/année sur 2 chiffres)\n",
    "            '%d/%m/%Y %H:%M:%S',\n",
    "            '%d/%m/%y %H:%M:%S',\n",
    "            # Formats sans zéros de remplissage (pour \"14/2/25 6:08\")\n",
    "            '%d/%-m/%y %-H:%M',  # Linux/Mac\n",
    "            '%d/%m/%y %H:%M',    # Windows (essai alternatif)\n",
    "            # Autres formats possibles\n",
    "            '%Y-%m-%dT%H:%M:%S.%f',  # Format ISO avec millisecondes\n",
    "            '%Y-%m-%dT%H:%M:%S',     # Format ISO sans millisecondes\n",
    "            '%Y-%m-%d %H:%M:%S'\n",
    "        ]\n",
    "        \n",
    "        # Essayer tous les formats explicites d'abord\n",
    "        for fmt in formats_to_try:\n",
    "            try:\n",
    "                # Adaptation pour Windows qui ne supporte pas %-\n",
    "                if '%-' in fmt and '/' in date_str:\n",
    "                    # Extraire les parties de la date pour un traitement manuel\n",
    "                    parts = date_str.split()\n",
    "                    if len(parts) == 2:  # Format \"14/2/25 6:08\"\n",
    "                        date_part = parts[0]\n",
    "                        time_part = parts[1]\n",
    "                        \n",
    "                        # Découper les composants de la date\n",
    "                        day, month, year = date_part.split('/')\n",
    "                        hour, minute = time_part.split(':')\n",
    "                        \n",
    "                        # Convertir en nombres\n",
    "                        day = int(day)\n",
    "                        month = int(month)\n",
    "                        \n",
    "                        # Déterminer si l'année est sur 2 ou 4 chiffres\n",
    "                        if len(year) == 2:\n",
    "                            # Convertir année sur 2 chiffres (20xx)\n",
    "                            year = 2000 + int(year)\n",
    "                        else:\n",
    "                            year = int(year)\n",
    "                            \n",
    "                        hour = int(hour)\n",
    "                        minute = int(minute)\n",
    "                        \n",
    "                        # Créer l'objet datetime\n",
    "                        return datetime(year, month, day, hour, minute)\n",
    "                else:\n",
    "                    return datetime.strptime(date_str, fmt)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Si aucun format explicite ne fonctionne, essayer avec dateutil.parser\n",
    "        try:\n",
    "            # Pour les formats JJ/MM/YYYY ou JJ/MM/YY, utiliser dayfirst=True\n",
    "            if '/' in date_str and len(date_str.split('/')[0]) <= 2:\n",
    "                # Exemple: \"14/2/25 6:08\" ou \"08/05/2025 01:04\"\n",
    "                return parser.parse(date_str, dayfirst=True)\n",
    "            else:\n",
    "                return parser.parse(date_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Échec du parsing avec dateutil: {date_str} - {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du parsing de la date '{date_str}': {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Charger les données\n",
    "def charger_donnees():\n",
    "    \"\"\"Charge et prépare les données sismiques avec gestion robuste des dates\"\"\"\n",
    "    # Essayer d'abord avec le nom de fichier original\n",
    "    try:\n",
    "        df = pd.read_csv('NewDataseisme_corrige.csv', sep=';')\n",
    "    except FileNotFoundError:\n",
    "        # Essayer avec le nom alternatif qui apparaît dans le code\n",
    "        try:\n",
    "            df = pd.read_csv('NewData-seisme_corrige.csv', sep=';')\n",
    "        except FileNotFoundError:\n",
    "            # Si les deux échouent, essayer sans tiret et avec séparateur virgule\n",
    "            try:\n",
    "                df = pd.read_csv('NewDataseisme_corrige.csv', sep=',')\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la lecture du fichier: {e}\")\n",
    "                print(\"Fichiers essayés: 'NewDataseisme_corrige.csv' et 'NewData-seisme_corrige.csv'\")\n",
    "                print(\"Vérifiez que le fichier existe et que le nom est correct.\")\n",
    "                return None\n",
    "    \n",
    "    # Afficher un aperçu\n",
    "    print(\"Aperçu des données:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Vérifier le format des dates (afficher quelques exemples)\n",
    "    print(\"\\nExemples de format de date dans les données:\")\n",
    "    if 'Date' in df.columns:\n",
    "        date_samples = df['Date'].head(5).tolist()\n",
    "        for sample in date_samples:\n",
    "            print(sample)\n",
    "    \n",
    "    # Convertir la colonne date avec notre fonction robuste\n",
    "    if 'Date' in df.columns:\n",
    "        print(\"\\nConversion des dates avec parse_date_flexible...\")\n",
    "        \n",
    "        # Créer une nouvelle colonne pour les dates converties\n",
    "        df['Date_dt'] = df['Date'].apply(parse_date_flexible)\n",
    "        \n",
    "        # Vérifier le succès de la conversion\n",
    "        success_count = df['Date_dt'].notna().sum()\n",
    "        fail_count = df['Date_dt'].isna().sum()\n",
    "        \n",
    "        print(f\"Conversion des dates: {success_count} succès, {fail_count} échecs\")\n",
    "        \n",
    "        if fail_count > 0:\n",
    "            print(f\"Attention: {fail_count} dates n'ont pas pu être converties et seront traitées comme manquantes\")\n",
    "        \n",
    "        # Si la conversion de date a réussi, utiliser la colonne Date_dt\n",
    "        if success_count > 0:\n",
    "            # Remplacer la colonne Date par Date_dt\n",
    "            df['Date_orig'] = df['Date']  # Garder l'original par sécurité\n",
    "            df['Date'] = df['Date_dt']\n",
    "        else:\n",
    "            print(\"ERREUR: Aucune date n'a pu être convertie correctement\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"ERREUR: Colonne 'Date' non trouvée dans les données\")\n",
    "        return None\n",
    "    \n",
    "    # Corriger le format des nombres (virgule à point)\n",
    "    for col in ['Magnitude', 'Latitude', 'Longitude', 'Profondeur']:\n",
    "        if col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "    \n",
    "    # Extraire les composantes temporelles\n",
    "    df['Annee'] = df['Date'].dt.year\n",
    "    df['Mois'] = df['Date'].dt.month\n",
    "    df['Jour'] = df['Date'].dt.day\n",
    "    df['Heure'] = df['Date'].dt.hour\n",
    "    df['JourSemaine'] = df['Date'].dt.dayofweek  # 0=Lundi, 6=Dimanche\n",
    "    df['Trimestre'] = df['Date'].dt.quarter\n",
    "    \n",
    "    # Ajouter ces composantes supplémentaires spécifiques à l'analyse des tendances\n",
    "    try:\n",
    "        df['Semaine'] = df['Date'].dt.isocalendar().week\n",
    "    except AttributeError:\n",
    "        # Pour les versions plus anciennes de pandas\n",
    "        df['Semaine'] = df['Date'].dt.week\n",
    "    \n",
    "    df['JourAnnee'] = df['Date'].dt.dayofyear\n",
    "    \n",
    "    # Définir les saisons: Printemps (3-5), Été (6-8), Automne (9-11), Hiver (12,1,2)\n",
    "    seasons = {\n",
    "        'Hiver': [12, 1, 2],\n",
    "        'Printemps': [3, 4, 5],\n",
    "        'Été': [6, 7, 8],\n",
    "        'Automne': [9, 10, 11]\n",
    "    }\n",
    "    \n",
    "    # Créer une colonne 'Saison'\n",
    "    def get_season(month):\n",
    "        for season, months in seasons.items():\n",
    "            if month in months:\n",
    "                return season\n",
    "    \n",
    "    df['Saison'] = df['Mois'].apply(get_season)\n",
    "    \n",
    "    # Nettoyer les colonnes (supprimer les caractères \\r si présents)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.replace('\\r', '')\n",
    "    \n",
    "    print(f\"Données chargées: {len(df)} enregistrements\")\n",
    "    \n",
    "    if not df['Date'].isna().all():\n",
    "        print(f\"Période couverte: {df['Date'].min().date()} à {df['Date'].max().date()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Fonction pour appliquer les filtres aux données\n",
    "def appliquer_filtres(df, annee_range, mois_selected, magnitude_range, profondeur_range):\n",
    "    \"\"\"Applique les filtres sélectionnés au dataframe\"\"\"\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    # Filtrer par année\n",
    "    df_filtered = df_filtered[(df_filtered['Annee'] >= annee_range[0]) & \n",
    "                            (df_filtered['Annee'] <= annee_range[1])]\n",
    "    \n",
    "    # Filtrer par mois\n",
    "    df_filtered = df_filtered[df_filtered['Mois'].isin(mois_selected)]\n",
    "    \n",
    "    # Filtrer par magnitude\n",
    "    df_filtered = df_filtered[(df_filtered['Magnitude'] >= magnitude_range[0]) & \n",
    "                            (df_filtered['Magnitude'] <= magnitude_range[1])]\n",
    "    \n",
    "    # Filtrer par profondeur\n",
    "    df_filtered = df_filtered[(df_filtered['Profondeur'] >= profondeur_range[0]) & \n",
    "                            (df_filtered['Profondeur'] <= profondeur_range[1])]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# 1. Analyse des tendances saisonnières\n",
    "# Remplacez la fonction analyser_tendances_saisonnieres par celle-ci\n",
    "def analyser_tendances_saisonnieres(df_filtered):\n",
    "    \"\"\"Analyse les tendances saisonnières dans les données sismiques\"\"\"\n",
    "    # Noms des mois\n",
    "    mois_noms = ['Janvier', 'Février', 'Mars', 'Avril', 'Mai', 'Juin', \n",
    "                 'Juillet', 'Août', 'Septembre', 'Octobre', 'Novembre', 'Décembre']\n",
    "    \n",
    "    # 1. Nombre de séismes par mois de l'année (tous les ans confondus)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    mois_counts = df_filtered.groupby('Mois').size()\n",
    "    \n",
    "    # Créer un dictionnaire pour tous les mois (même ceux sans données)\n",
    "    mois_dict = {i: 0 for i in range(1, 13)}\n",
    "    for mois, count in mois_counts.items():\n",
    "        mois_dict[mois] = count\n",
    "    \n",
    "    # Utiliser le dictionnaire pour le graphique\n",
    "    plt.bar(range(1, 13), [mois_dict[i] for i in range(1, 13)])\n",
    "    plt.title('Nombre de séismes par mois (toutes années confondues)')\n",
    "    plt.xlabel('Mois')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.xticks(range(1, 13), mois_noms, rotation=45)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test statistique pour vérifier si la distribution mensuelle est uniforme\n",
    "    if len(mois_counts) == 12:  # Seulement si tous les mois sont présents\n",
    "        chi2, p = stats.chisquare(mois_counts)\n",
    "        print(f\"Test Chi² d'uniformité de la distribution mensuelle: chi²={chi2:.2f}, p-value={p:.4f}\")\n",
    "        if p < 0.05:\n",
    "            print(\"Il existe une variation saisonnière statistiquement significative dans le nombre de séismes par mois.\")\n",
    "        else:\n",
    "            print(\"La distribution mensuelle des séismes semble uniforme (pas de tendance saisonnière significative).\")\n",
    "    else:\n",
    "        print(\"Test Chi² non effectué car tous les mois ne sont pas représentés dans les données filtrées.\")\n",
    "    \n",
    "    # 2. Heatmap des séismes par mois et par année si plusieurs années\n",
    "    if len(df_filtered['Annee'].unique()) > 1:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Créer un DataFrame avec tous les mois et années possibles\n",
    "        annees = sorted(df_filtered['Annee'].unique())\n",
    "        \n",
    "        # Préparer les données pour la heatmap\n",
    "        try:\n",
    "            heatmap_data = df_filtered.groupby(['Annee', 'Mois']).size().unstack(fill_value=0)\n",
    "            \n",
    "            # S'assurer que toutes les colonnes (mois) existent\n",
    "            for m in range(1, 13):\n",
    "                if m not in heatmap_data.columns:\n",
    "                    heatmap_data[m] = 0\n",
    "            \n",
    "            # Trier les colonnes pour avoir l'ordre des mois correct\n",
    "            heatmap_data = heatmap_data.reindex(sorted(heatmap_data.columns), axis=1)\n",
    "            \n",
    "            # Création d'une colormap personnalisée\n",
    "            colors = [\"#f7fbff\", \"#deebf7\", \"#c6dbef\", \"#9ecae1\", \"#6baed6\", \"#4292c6\", \"#2171b5\", \"#08519c\", \"#08306b\"]\n",
    "            cmap = LinearSegmentedColormap.from_list(\"custom_blues\", colors)\n",
    "            \n",
    "            # Afficher la heatmap\n",
    "            ax = sns.heatmap(heatmap_data, cmap=cmap, annot=True, fmt=\"d\", linewidths=.5)\n",
    "            plt.title('Nombre de séismes par mois et par année')\n",
    "            plt.xlabel('Mois')\n",
    "            plt.ylabel('Année')\n",
    "            \n",
    "            # Configurer les étiquettes d'axe\n",
    "            ax.set_xticklabels([mois_noms[i-1] for i in heatmap_data.columns], rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Impossible de créer la heatmap: {e}\")\n",
    "            print(\"Cela peut être dû à un nombre insuffisant de données après filtrage.\")\n",
    "    \n",
    "    # 3. Évolution de la magnitude moyenne par mois\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    mag_means = df_filtered.groupby('Mois')['Magnitude'].mean()\n",
    "    \n",
    "    # Créer un dictionnaire pour tous les mois (même ceux sans données)\n",
    "    mag_dict = {i: 0 for i in range(1, 13)}\n",
    "    for mois, mean in mag_means.items():\n",
    "        mag_dict[mois] = mean\n",
    "    \n",
    "    # Utiliser uniquement les mois pour lesquels on a des données\n",
    "    mois_avec_donnees = sorted(mag_means.index)\n",
    "    plt.bar(mois_avec_donnees, [mag_dict[m] for m in mois_avec_donnees], color='orange')\n",
    "    plt.title('Magnitude moyenne des séismes par mois')\n",
    "    plt.xlabel('Mois')\n",
    "    plt.ylabel('Magnitude moyenne')\n",
    "    plt.xticks(mois_avec_donnees, [mois_noms[i-1] for i in mois_avec_donnees], rotation=45)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Analyse par saison\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    season_counts = df_filtered.groupby('Saison').size()\n",
    "    \n",
    "    # Réorganiser les saisons dans l'ordre chronologique\n",
    "    season_order = ['Hiver', 'Printemps', 'Été', 'Automne']\n",
    "    season_counts = season_counts.reindex([s for s in season_order if s in season_counts.index])\n",
    "    \n",
    "    plt.bar(season_counts.index, season_counts.values, color='purple')\n",
    "    plt.title('Nombre de séismes par saison')\n",
    "    plt.xlabel('Saison')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher les statistiques par saison\n",
    "    if len(season_counts) > 0:\n",
    "        print(\"\\nRépartition des séismes par saison:\")\n",
    "        for saison, count in season_counts.items():\n",
    "            print(f\"{saison}: {count} séismes ({count/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "# 2. Analyse des tendances journalières\n",
    "def analyser_tendances_journalieres(df_filtered):\n",
    "    \"\"\"Analyse les tendances journalières dans les données sismiques\"\"\"\n",
    "    \n",
    "    # 1. Distribution des séismes par heure\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    heure_counts = df_filtered.groupby('Heure').size()\n",
    "    \n",
    "    plt.bar(heure_counts.index, heure_counts.values, color='darkblue')\n",
    "    plt.title('Nombre de séismes par heure de la journée')\n",
    "    plt.xlabel('Heure')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Diviser la journée en 4 périodes de 6h\n",
    "    periodes = {\n",
    "        'Nuit (0h-6h)': list(range(0, 6)),\n",
    "        'Matin (6h-12h)': list(range(6, 12)),\n",
    "        'Après-midi (12h-18h)': list(range(12, 18)),\n",
    "        'Soir (18h-24h)': list(range(18, 24))\n",
    "    }\n",
    "    \n",
    "    periode_counts = {}\n",
    "    for nom, heures in periodes.items():\n",
    "        periode_counts[nom] = df_filtered[df_filtered['Heure'].isin(heures)].shape[0]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.bar(periode_counts.keys(), periode_counts.values(), color='navy')\n",
    "    plt.title('Nombre de séismes par période de la journée')\n",
    "    plt.xlabel('Période')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Distribution par jour de la semaine\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    jours_semaine = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']\n",
    "    jour_counts = df_filtered.groupby('JourSemaine').size()\n",
    "    \n",
    "    # Créer un dictionnaire pour tous les jours (même ceux sans données)\n",
    "    jours_dict = {i: 0 for i in range(7)}\n",
    "    for jour, count in jour_counts.items():\n",
    "        jours_dict[jour] = count\n",
    "    \n",
    "    plt.bar(range(7), [jours_dict.get(i, 0) for i in range(7)], color='darkgreen')\n",
    "    plt.title('Nombre de séismes par jour de la semaine')\n",
    "    plt.xlabel('Jour')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.xticks(range(7), jours_semaine)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Comparer jours de semaine vs weekend\n",
    "    semaine = df_filtered[df_filtered['JourSemaine'] < 5].shape[0]\n",
    "    weekend = df_filtered[df_filtered['JourSemaine'] >= 5].shape[0]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(['Jours de semaine (Lun-Ven)', 'Weekend (Sam-Dim)'], [semaine, weekend], color=['blue', 'red'])\n",
    "    plt.title('Nombre de séismes : Jours de semaine vs Weekend')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher les statistiques\n",
    "    total = semaine + weekend\n",
    "    if total > 0:\n",
    "        print(\"\\nRépartition des séismes entre semaine et weekend:\")\n",
    "        print(f\"Jours de semaine (Lun-Ven): {semaine} séismes ({semaine/total*100:.1f}%)\")\n",
    "        print(f\"Weekend (Sam-Dim): {weekend} séismes ({weekend/total*100:.1f}%)\")\n",
    "        print(f\"Rapport observé: {weekend/semaine:.2f} (weekend/semaine)\")\n",
    "        print(f\"Rapport attendu si distribution uniforme: {2/5:.2f} (2 jours / 5 jours)\")\n",
    "\n",
    "# 3. Analyse des tendances à long terme\n",
    "def analyser_tendances_long_terme(df_filtered):\n",
    "    \"\"\"Analyse les tendances à long terme dans les données sismiques\"\"\"\n",
    "    \n",
    "    # Vérifier si nous avons suffisamment d'années pour cette analyse\n",
    "    annees_uniques = df_filtered['Annee'].unique()\n",
    "    if len(annees_uniques) < 2:\n",
    "        print(\"Cette analyse nécessite des données sur au moins 2 années différentes.\")\n",
    "        print(f\"Années disponibles dans la sélection: {', '.join(map(str, sorted(annees_uniques)))}\")\n",
    "        return\n",
    "    \n",
    "    # Regrouper les données par mois pour l'analyse des séries temporelles\n",
    "    df_mensuel = df_filtered.groupby(pd.Grouper(key='Date', freq='M')).agg({\n",
    "        'Magnitude': ['count', 'mean', 'max'],\n",
    "        'Profondeur': 'mean'\n",
    "    })\n",
    "    \n",
    "    df_mensuel.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in df_mensuel.columns.values]\n",
    "    df_mensuel.rename(columns={'Magnitude_count': 'Nombre_Seismes'}, inplace=True)\n",
    "    \n",
    "    # 1. Nombre de séismes par mois au fil du temps\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(df_mensuel.index, df_mensuel['Nombre_Seismes'], marker='o', linestyle='-', color='blue')\n",
    "    \n",
    "    # Ajout d'une ligne de tendance avec régression linéaire\n",
    "    if len(df_mensuel) > 1:\n",
    "        X = np.arange(len(df_mensuel)).reshape(-1, 1)\n",
    "        y = df_mensuel['Nombre_Seismes'].values\n",
    "        \n",
    "        model = stats.linregress(X.flatten(), y)\n",
    "        trend_line = model.slope * X.flatten() + model.intercept\n",
    "        plt.plot(df_mensuel.index, trend_line, color='red', linestyle='--', \n",
    "                label=f'Tendance (pente={model.slope:.4f}, p={model.pvalue:.4f})')\n",
    "        \n",
    "        if model.pvalue < 0.05:\n",
    "            trend_direction = \"augmentation\" if model.slope > 0 else \"diminution\"\n",
    "            print(f\"Il existe une tendance significative à la {trend_direction} du nombre de séismes au fil du temps.\")\n",
    "            print(f\"Pente de la tendance: {model.slope:.4f} séismes/mois, p-value: {model.pvalue:.4f}\")\n",
    "        else:\n",
    "            print(\"Aucune tendance significative n'a été détectée dans le nombre de séismes au fil du temps.\")\n",
    "    \n",
    "    # Ajouter une courbe lissée (moyenne mobile)\n",
    "    if len(df_mensuel) >= 6:  # Au moins 6 mois pour calculer une moyenne mobile\n",
    "        window_size = min(6, len(df_mensuel) // 2)  # Utiliser un minimum de 6 mois ou la moitié des données disponibles\n",
    "        rolling_mean = df_mensuel['Nombre_Seismes'].rolling(window=window_size, center=True).mean()\n",
    "        plt.plot(df_mensuel.index, rolling_mean, color='green', linestyle='-.',\n",
    "                label=f'Moyenne mobile ({window_size} mois)')\n",
    "    \n",
    "    plt.title('Évolution du nombre de séismes par mois')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Magnitude moyenne par mois au fil du temps\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(df_mensuel.index, df_mensuel['Magnitude_mean'], marker='o', linestyle='-', color='orange')\n",
    "    \n",
    "    # Ajouter une ligne de tendance\n",
    "    if len(df_mensuel) > 1:\n",
    "        X = np.arange(len(df_mensuel)).reshape(-1, 1)\n",
    "        y = df_mensuel['Magnitude_mean'].values\n",
    "        model_mag = stats.linregress(X.flatten(), y)\n",
    "        trend_line_mag = model_mag.slope * X.flatten() + model_mag.intercept\n",
    "        plt.plot(df_mensuel.index, trend_line_mag, color='red', linestyle='--', \n",
    "                label=f'Tendance (pente={model_mag.slope:.4f}, p={model_mag.pvalue:.4f})')\n",
    "        \n",
    "        if model_mag.pvalue < 0.05:\n",
    "            trend_direction = \"augmentation\" if model_mag.slope > 0 else \"diminution\"\n",
    "            print(f\"Il existe une tendance significative à la {trend_direction} de la magnitude moyenne au fil du temps.\")\n",
    "            print(f\"Pente de la tendance: {model_mag.slope:.4f} magnitude/mois, p-value: {model_mag.pvalue:.4f}\")\n",
    "        else:\n",
    "            print(\"Aucune tendance significative n'a été détectée dans la magnitude moyenne au fil du temps.\")\n",
    "    \n",
    "    plt.title('Évolution de la magnitude moyenne par mois')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Magnitude moyenne')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Analyse des tendances par année - tableau récapitulatif\n",
    "    df_annuel = df_filtered.groupby('Annee').agg({\n",
    "        'Magnitude': ['count', 'mean', 'max', 'min', 'std'],\n",
    "        'Profondeur': ['mean', 'min', 'max', 'std']\n",
    "    })\n",
    "    \n",
    "    df_annuel.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in df_annuel.columns.values]\n",
    "    \n",
    "    print(\"\\nRésumé annuel de l'activité sismique:\")\n",
    "    display(df_annuel)\n",
    "\n",
    "# 4. Analyse des cycles et périodicités\n",
    "def analyser_cycles_periodicites(df_filtered):\n",
    "    \"\"\"Analyse les cycles et périodicités potentiels dans les données sismiques\"\"\"\n",
    "    \n",
    "    # Vérifier si nous avons suffisamment de données pour cette analyse\n",
    "    if len(df_filtered) < 100:\n",
    "        print(\"Cette analyse nécessite un grand nombre de données (>100 séismes).\")\n",
    "        print(f\"Nombre d'enregistrements dans la sélection actuelle: {len(df_filtered)}\")\n",
    "        return\n",
    "    \n",
    "    # Créer une série temporelle pour l'analyse de périodicité\n",
    "    # Agréger par jour pour avoir une série temporelle régulière\n",
    "    ts_daily = df_filtered.groupby(df_filtered['Date'].dt.date).size()\n",
    "    date_range = pd.date_range(start=ts_daily.index.min(), end=ts_daily.index.max())\n",
    "    ts_daily = ts_daily.reindex(date_range, fill_value=0)\n",
    "    \n",
    "    # 1. Autocorrélation - détection de périodicité\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    pd.plotting.autocorrelation_plot(ts_daily)\n",
    "    plt.title('Autocorrélation du nombre de séismes par jour')\n",
    "    plt.xlim(0, min(100, len(ts_daily)))  # Limiter à 100 lags pour mieux voir les cycles courts\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"L'autocorrélation permet de détecter des cycles périodiques dans les données.\")\n",
    "    print(\"Les pics dans le graphique suggèrent des périodicités possibles.\")\n",
    "    \n",
    "    # 2. Analyse de la périodicité hebdomadaire\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    jour_semaine_counts = df_filtered.groupby('JourSemaine').size()\n",
    "    \n",
    "    # Créer un dictionnaire pour tous les jours (même ceux sans données)\n",
    "    jours_dict = {i: 0 for i in range(7)}\n",
    "    for jour, count in jour_semaine_counts.items():\n",
    "        jours_dict[jour] = count\n",
    "    \n",
    "    jours_semaine = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']\n",
    "    plt.bar(range(7), [jours_dict.get(i, 0) for i in range(7)], color='purple')\n",
    "    plt.title('Nombre de séismes par jour de la semaine (cycle hebdomadaire)')\n",
    "    plt.xlabel('Jour de la semaine')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.xticks(range(7), jours_semaine)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Analyse par saison (pour détecter des cycles annuels)\n",
    "    if len(df_filtered['Annee'].unique()) >= 1:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        # Compter les séismes par jour de l'année\n",
    "        jour_annee_counts = df_filtered.groupby('JourAnnee').size()\n",
    "        \n",
    "        plt.plot(jour_annee_counts.index, jour_annee_counts.values)\n",
    "        plt.title('Nombre de séismes par jour de l\\'année (cycles annuels)')\n",
    "        plt.xlabel('Jour de l\\'année')\n",
    "        plt.ylabel('Nombre de séismes')\n",
    "        \n",
    "        # Répartir les jours par mois pour l'affichage\n",
    "        mois_jours = [(1, 31), (2, 28), (3, 31), (4, 30), (5, 31), (6, 30), \n",
    "                    (7, 31), (8, 31), (9, 30), (10, 31), (11, 30), (12, 31)]\n",
    "        mois_limites = [1]\n",
    "        for m, j in mois_jours:\n",
    "            mois_limites.append(mois_limites[-1] + j)\n",
    "        \n",
    "        # Ajouter des lignes verticales pour séparer les mois\n",
    "        mois_noms = ['Janvier', 'Février', 'Mars', 'Avril', 'Mai', 'Juin', \n",
    "                    'Juillet', 'Août', 'Septembre', 'Octobre', 'Novembre', 'Décembre']\n",
    "        for i, limite in enumerate(mois_limites[:-1]):\n",
    "            plt.axvline(x=limite, color='gray', linestyle='--', alpha=0.5)\n",
    "            if jour_annee_counts.max() > 0:\n",
    "                plt.text(limite + (mois_limites[i+1] - limite)/2, jour_annee_counts.max() * 0.9, \n",
    "                        mois_noms[i], ha='center')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Création du Dashboard\n",
    "def creer_dashboard():\n",
    "    # Charger les données\n",
    "    df = charger_donnees()\n",
    "    \n",
    "    # Créer les widgets pour les filtres\n",
    "    output_dashboard = widgets.Output()\n",
    "    \n",
    "    # Filtres temporels\n",
    "    annees = sorted(df['Annee'].unique())\n",
    "    mois = list(range(1, 13))\n",
    "    mois_noms = ['Janvier', 'Février', 'Mars', 'Avril', 'Mai', 'Juin', \n",
    "                'Juillet', 'Août', 'Septembre', 'Octobre', 'Novembre', 'Décembre']\n",
    "    mois_dict = {i+1: nom for i, nom in enumerate(mois_noms)}\n",
    "    \n",
    "    # Filtres magnitude et profondeur\n",
    "    min_mag = float(df['Magnitude'].min())\n",
    "    max_mag = float(df['Magnitude'].max())\n",
    "    min_prof = float(df['Profondeur'].min())\n",
    "    max_prof = float(df['Profondeur'].max())\n",
    "    \n",
    "    # Création des widgets\n",
    "    annee_slider = widgets.IntRangeSlider(\n",
    "        value=[annees[0], annees[-1]],\n",
    "        min=annees[0],\n",
    "        max=annees[-1],\n",
    "        step=1,\n",
    "        description='Années:',\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width='70%')\n",
    "    )\n",
    "    \n",
    "    mois_checkbox = widgets.SelectMultiple(\n",
    "        options=[(mois_dict[m], m) for m in mois],\n",
    "        value=mois,\n",
    "        description='Mois:',\n",
    "        layout=widgets.Layout(width='50%', height='100px')\n",
    "    )\n",
    "    \n",
    "    magnitude_slider = widgets.FloatRangeSlider(\n",
    "        value=[min_mag, max_mag],\n",
    "        min=min_mag,\n",
    "        max=max_mag,\n",
    "        step=0.1,\n",
    "        description='Magnitude:',\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width='70%')\n",
    "    )\n",
    "    \n",
    "    profondeur_slider = widgets.FloatRangeSlider(\n",
    "        value=[min_prof, max_prof],\n",
    "        min=min_prof,\n",
    "        max=max_prof,\n",
    "        step=5,\n",
    "        description='Profondeur:',\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width='70%')\n",
    "    )\n",
    "    \n",
    "    # Types d'analyse\n",
    "    analyse_type = widgets.RadioButtons(\n",
    "        options=['Tendances saisonnières', 'Tendances journalières', \n",
    "                'Tendances à long terme', 'Cycles et périodicités'],\n",
    "        description='Type d\\'analyse:',\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "    \n",
    "    filtrer_button = widgets.Button(\n",
    "        description='Analyser les tendances',\n",
    "        button_style='primary',\n",
    "        tooltip='Cliquez pour analyser les tendances',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    \n",
    "    # Fonction principale pour mettre à jour l'analyse\n",
    "    def update_dashboard(b):\n",
    "        with output_dashboard:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Récupérer les valeurs des filtres\n",
    "            annee_range = annee_slider.value\n",
    "            mois_selected = mois_checkbox.value\n",
    "            magnitude_range = magnitude_slider.value\n",
    "            profondeur_range = profondeur_slider.value\n",
    "            \n",
    "            # Appliquer les filtres\n",
    "            df_filtered = appliquer_filtres(df, annee_range, mois_selected, magnitude_range, profondeur_range)\n",
    "            \n",
    "            print(f\"Données filtrées: {len(df_filtered)} séismes sur {len(df)} ({len(df_filtered)/len(df)*100:.1f}%)\")\n",
    "            \n",
    "            if len(df_filtered) == 0:\n",
    "                print(\"Aucune donnée ne correspond aux critères de filtrage!\")\n",
    "                return\n",
    "            \n",
    "            # Effectuer l'analyse sélectionnée\n",
    "            analysis_type = analyse_type.value\n",
    "            \n",
    "            if analysis_type == 'Tendances saisonnières':\n",
    "                analyser_tendances_saisonnieres(df_filtered)\n",
    "                \n",
    "            elif analysis_type == 'Tendances journalières':\n",
    "                analyser_tendances_journalieres(df_filtered)\n",
    "                \n",
    "            elif analysis_type == 'Tendances à long terme':\n",
    "                analyser_tendances_long_terme(df_filtered)\n",
    "                \n",
    "            elif analysis_type == 'Cycles et périodicités':\n",
    "                analyser_cycles_periodicites(df_filtered)\n",
    "    \n",
    "    # Connecter le bouton à la fonction d'actualisation\n",
    "    filtrer_button.on_click(update_dashboard)\n",
    "    \n",
    "     # Afficher l'interface\n",
    "    print(\"\\n--- DASHBOARD D'ANALYSE DES TENDANCES SISMIQUES ---\")\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HBox([widgets.Label('Filtres temporels', style={'font_weight': 'bold'})]),\n",
    "        widgets.HBox([annee_slider]),\n",
    "        widgets.HBox([mois_checkbox]),\n",
    "        widgets.HBox([widgets.Label('Filtres de caractéristiques', style={'font_weight': 'bold'})]),\n",
    "        widgets.HBox([magnitude_slider]),\n",
    "        widgets.HBox([profondeur_slider]),\n",
    "        widgets.HBox([widgets.Label('Type d\\'analyse', style={'font_weight': 'bold'})]),\n",
    "        widgets.HBox([analyse_type]),\n",
    "        widgets.HBox([filtrer_button])\n",
    "    ])\n",
    "    \n",
    "    display(controls)\n",
    "    display(output_dashboard)\n",
    "    \n",
    "    # Lancer l'analyse avec les filtres par défaut\n",
    "    update_dashboard(None)\n",
    "\n",
    "# Lancer le dashboard\n",
    "if __name__ == \"__main__\":\n",
    "    creer_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49dd8120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentative de chargement du fichier: NewDataseisme_corriger.csv\n",
      "Fichier non trouvé: NewDataseisme_corriger.csv\n",
      "Tentative de chargement du fichier: NewDataseisme_corrige.csv\n",
      "Fichier chargé avec succès: NewDataseisme_corrige.csv\n",
      "\n",
      "Aperçu des données:\n",
      "               Date    Magnitude    Latitude  Longitude Profondeur  origine\n",
      "0  08/05/2025 20:34  1,998769442  -12,750200  45,561000      51,54        5\n",
      "1  08/05/2025 20:27  2,374112553  -12,788200  45,619300      47,24        5\n",
      "2  08/05/2025 19:16  1,667391108  -12,566000  45,175300      38,39        5\n",
      "3  08/05/2025 01:04  2,025011775  -12,805300  45,580500      49,82        5\n",
      "4  07/05/2025 18:46  1,282582543  -12,805500  45,347200      43,46        5\n",
      "\n",
      "Exemples de format de date dans les données:\n",
      "08/05/2025 20:34\n",
      "08/05/2025 20:27\n",
      "08/05/2025 19:16\n",
      "08/05/2025 01:04\n",
      "07/05/2025 18:46\n",
      "\n",
      "Conversion des dates avec parse_date_flexible...\n",
      "Conversion des dates: 14216 succès, 0 échecs\n",
      "\n",
      "Attention: 6 valeurs de profondeur négatives détectées.\n",
      "Application de la valeur absolue pour ces valeurs...\n",
      "Données chargées: 14216 enregistrements\n",
      "Période couverte: 2018-05-10 à 2025-05-08\n",
      "\n",
      "--- DASHBOARD D'ANALYSE DES CARACTÉRISTIQUES SISMIQUES ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3125c48c641d46d6913bd56edcaa5b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Accordion(children=(VBox(children=(HBox(children=(IntRangeSlider(value=(2018, 2025), continuous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aba5da3b4b842e9a32f75cef7c69954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######## ANALYSE DES CARACTÉRISTIQUES SISMIQUES   ##########\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def parse_date_flexible(date_str):\n",
    "    \"\"\"\n",
    "    Fonction robuste pour parser différents formats de date\n",
    "    Gère spécifiquement les formats identifiés dans vos données comme \n",
    "    \"08/05/2025 01:04\" et \"14/2/25 6:08\"\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    from dateutil import parser\n",
    "    import pandas as pd\n",
    "    \n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Si c'est déjà un datetime\n",
    "        if isinstance(date_str, datetime):\n",
    "            return date_str\n",
    "            \n",
    "        # Nettoyer la chaîne si nécessaire\n",
    "        date_str = str(date_str).strip()\n",
    "        \n",
    "        # Formats détectés dans vos données\n",
    "        formats_to_try = [\n",
    "            '%d/%m/%Y %H:%M',    # Pour \"08/05/2025 01:04\" (jour/mois/année sur 4 chiffres)\n",
    "            '%d/%m/%y %H:%M',    # Pour \"08/05/25 01:04\" (jour/mois/année sur 2 chiffres)\n",
    "            '%d/%m/%Y %H:%M:%S',\n",
    "            '%d/%m/%y %H:%M:%S',\n",
    "            # Formats sans zéros de remplissage (pour \"14/2/25 6:08\")\n",
    "            '%d/%-m/%y %-H:%M',  # Linux/Mac\n",
    "            '%d/%m/%y %H:%M',    # Windows (essai alternatif)\n",
    "            # Autres formats possibles\n",
    "            '%Y-%m-%dT%H:%M:%S.%f',  # Format ISO avec millisecondes\n",
    "            '%Y-%m-%dT%H:%M:%S',     # Format ISO sans millisecondes\n",
    "            '%Y-%m-%d %H:%M:%S'\n",
    "        ]\n",
    "        \n",
    "        # Essayer tous les formats explicites d'abord\n",
    "        for fmt in formats_to_try:\n",
    "            try:\n",
    "                # Adaptation pour Windows qui ne supporte pas %-\n",
    "                if '%-' in fmt and '/' in date_str:\n",
    "                    # Extraire les parties de la date pour un traitement manuel\n",
    "                    parts = date_str.split()\n",
    "                    if len(parts) == 2:  # Format \"14/2/25 6:08\"\n",
    "                        date_part = parts[0]\n",
    "                        time_part = parts[1]\n",
    "                        \n",
    "                        # Découper les composants de la date\n",
    "                        day, month, year = date_part.split('/')\n",
    "                        hour, minute = time_part.split(':')\n",
    "                        \n",
    "                        # Convertir en nombres\n",
    "                        day = int(day)\n",
    "                        month = int(month)\n",
    "                        \n",
    "                        # Déterminer si l'année est sur 2 ou 4 chiffres\n",
    "                        if len(year) == 2:\n",
    "                            # Convertir année sur 2 chiffres (20xx)\n",
    "                            year = 2000 + int(year)\n",
    "                        else:\n",
    "                            year = int(year)\n",
    "                            \n",
    "                        hour = int(hour)\n",
    "                        minute = int(minute)\n",
    "                        \n",
    "                        # Créer l'objet datetime\n",
    "                        return datetime(year, month, day, hour, minute)\n",
    "                else:\n",
    "                    return datetime.strptime(date_str, fmt)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Si aucun format explicite ne fonctionne, essayer avec dateutil.parser\n",
    "        try:\n",
    "            # Pour les formats JJ/MM/YYYY ou JJ/MM/YY, utiliser dayfirst=True\n",
    "            if '/' in date_str and len(date_str.split('/')[0]) <= 2:\n",
    "                # Exemple: \"14/2/25 6:08\" ou \"08/05/2025 01:04\"\n",
    "                return parser.parse(date_str, dayfirst=True)\n",
    "            else:\n",
    "                return parser.parse(date_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Échec du parsing avec dateutil: {date_str} - {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du parsing de la date '{date_str}': {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Charger les données\n",
    "# This is the corrected code section for handling negative depth values\n",
    "# When loading and preparing the data, modify the code to use absolute values\n",
    "# for profondeur (depth) before calculating potentiel_destructeur\n",
    "\n",
    "def charger_donnees(nom_fichier=None):\n",
    "    \"\"\"Charge et prépare les données sismiques avec correction des profondeurs négatives\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # [Le reste du code de chargement reste inchangé...]\n",
    "    \n",
    "    # Liste des noms de fichiers à essayer\n",
    "    fichiers_a_essayer = []\n",
    "    if nom_fichier:\n",
    "        fichiers_a_essayer.append(nom_fichier)\n",
    "    \n",
    "    # Ajouter les variantes possibles du nom de fichier\n",
    "    fichiers_a_essayer.extend([\n",
    "        'NewDataseisme_corriger.csv',\n",
    "        'NewDataseisme_corrige.csv',\n",
    "        'NewData-seisme_corriger.csv',\n",
    "        'NewData-seisme_corrige.csv'\n",
    "    ])\n",
    "    \n",
    "    # Essayer de charger chaque fichier avec différents séparateurs\n",
    "    df = None\n",
    "    for fichier in fichiers_a_essayer:\n",
    "        try:\n",
    "            print(f\"Tentative de chargement du fichier: {fichier}\")\n",
    "            df = pd.read_csv(fichier, sep=';')\n",
    "            print(f\"Fichier chargé avec succès: {fichier}\")\n",
    "            break\n",
    "        except FileNotFoundError:\n",
    "            try:\n",
    "                df = pd.read_csv(fichier, sep=',')\n",
    "                print(f\"Fichier chargé avec succès (séparateur virgule): {fichier}\")\n",
    "                break\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Fichier non trouvé: {fichier}\")\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement de {fichier}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Si aucun fichier n'a pu être chargé, créer des données factices pour démonstration\n",
    "    if df is None:\n",
    "        print(\"\\nAucun fichier de données trouvé. Création de données de démonstration...\")\n",
    "        # Créer des données aléatoires pour la démonstration\n",
    "        np.random.seed(42)  # Pour reproductibilité\n",
    "        n_samples = 1000\n",
    "        \n",
    "        # Dates entre 2018 et 2025\n",
    "        start_date = pd.Timestamp('2018-01-01')\n",
    "        end_date = pd.Timestamp('2025-05-01')\n",
    "        dates = pd.date_range(start=start_date, end=end_date, periods=n_samples)\n",
    "        \n",
    "        # Magnitudes entre 0.5 et 6.0, avec une distribution log-normale\n",
    "        magnitudes = np.random.lognormal(mean=0.5, sigma=0.5, size=n_samples)\n",
    "        magnitudes = np.clip(magnitudes, 0.5, 6.0)\n",
    "        \n",
    "        # Profondeurs entre 10 et 200 km\n",
    "        profondeurs = np.random.lognormal(mean=3.5, sigma=0.7, size=n_samples)\n",
    "        profondeurs = np.clip(profondeurs, 10, 200)\n",
    "        \n",
    "        # Créer le DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'Magnitude': magnitudes,\n",
    "            'Profondeur': profondeurs,\n",
    "            'Latitude': np.random.uniform(-12.8, -12.7, n_samples),\n",
    "            'Longitude': np.random.uniform(45.1, 45.2, n_samples)\n",
    "        })\n",
    "        \n",
    "        print(\"Données de démonstration créées avec succès.\")\n",
    "    \n",
    "    # Afficher un aperçu\n",
    "    print(\"\\nAperçu des données:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Corriger le format des nombres (virgule à point)\n",
    "    for col in ['Magnitude', 'Latitude', 'Longitude', 'Profondeur']:\n",
    "        if col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "    \n",
    "    # Convertir la colonne de date avec notre fonction robuste\n",
    "    if 'Date' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
    "        # Examiner les premières valeurs pour comprendre le format\n",
    "        date_samples = df['Date'].head(5).tolist()\n",
    "        print(\"\\nExemples de format de date dans les données:\")\n",
    "        for sample in date_samples:\n",
    "            print(sample)\n",
    "        \n",
    "        print(\"\\nConversion des dates avec parse_date_flexible...\")\n",
    "        # Appliquer la fonction de conversion\n",
    "        df['Date'] = df['Date'].apply(parse_date_flexible)\n",
    "        \n",
    "        # Vérifier le succès de la conversion\n",
    "        success_count = df['Date'].notna().sum()\n",
    "        fail_count = df['Date'].isna().sum()\n",
    "        \n",
    "        print(f\"Conversion des dates: {success_count} succès, {fail_count} échecs\")\n",
    "        \n",
    "        if fail_count > 0:\n",
    "            print(f\"Attention: {fail_count} dates n'ont pas pu être converties et seront traitées comme manquantes\")\n",
    "            \n",
    "        if success_count == 0:\n",
    "            print(\"ERREUR: Aucune date n'a pu être convertie correctement. Création de dates factices.\")\n",
    "            # Créer des dates factices\n",
    "            df['Date'] = pd.date_range(start='2018-01-01', periods=len(df), freq='D')\n",
    "            print(\"Dates factices créées pour permettre l'analyse.\")\n",
    "    \n",
    "    # Extraire les composantes temporelles\n",
    "    df['Annee'] = df['Date'].dt.year\n",
    "    df['Mois'] = df['Date'].dt.month\n",
    "    df['Jour'] = df['Date'].dt.day\n",
    "    df['Heure'] = df['Date'].dt.hour\n",
    "    \n",
    "    # Catégoriser les séismes selon leur magnitude\n",
    "    magnitude_categories = [\n",
    "        (0, 2.5, 'Micro'),\n",
    "        (2.5, 4.0, 'Faible'),\n",
    "        (4.0, 5.0, 'Léger'),\n",
    "        (5.0, 6.0, 'Modéré'),\n",
    "        (6.0, 7.0, 'Fort'),\n",
    "        (7.0, 8.0, 'Majeur'),\n",
    "        (8.0, float('inf'), 'Grand')\n",
    "    ]\n",
    "    \n",
    "    def categorize_magnitude(mag):\n",
    "        for low, high, category in magnitude_categories:\n",
    "            if low <= mag < high:\n",
    "                return category\n",
    "        return 'Inconnu'\n",
    "    \n",
    "    df['Magnitude_Categorie'] = df['Magnitude'].apply(categorize_magnitude)\n",
    "    \n",
    "    # CORRECTION: Si des valeurs négatives de profondeur existent, prendre leur valeur absolue\n",
    "    if (df['Profondeur'] < 0).any():\n",
    "        print(f\"\\nAttention: {(df['Profondeur'] < 0).sum()} valeurs de profondeur négatives détectées.\")\n",
    "        print(\"Application de la valeur absolue pour ces valeurs...\")\n",
    "        df['Profondeur'] = df['Profondeur'].abs()\n",
    "    \n",
    "    # Catégoriser les séismes selon leur profondeur\n",
    "    depth_categories = [\n",
    "        (0, 70, 'Peu profond'),\n",
    "        (70, 300, 'Intermédiaire'),\n",
    "        (300, float('inf'), 'Profond')\n",
    "    ]\n",
    "    \n",
    "    def categorize_depth(depth):\n",
    "        for low, high, category in depth_categories:\n",
    "            if low <= depth < high:\n",
    "                return category\n",
    "        return 'Inconnu'\n",
    "    \n",
    "    df['Profondeur_Categorie'] = df['Profondeur'].apply(categorize_depth)\n",
    "    \n",
    "    # Calcul de l'énergie libérée (formule approximative basée sur la magnitude)\n",
    "    # Énergie en joules selon la formule E = 10^(1.5*M+4.8)\n",
    "    df['Energie'] = 10**(1.5 * df['Magnitude'] + 4.8)\n",
    "    \n",
    "    # Estimation du potentiel destructeur (combinaison de magnitude et profondeur inverse)\n",
    "    # Un séisme superficiel de grande magnitude est potentiellement plus destructeur\n",
    "    # Formule simplifiée: Magnitude * (1 + 70/profondeur) \n",
    "    # CORRECTION: Nous nous assurons d'utiliser des profondeurs positives\n",
    "    df['Potentiel_Destructeur'] = df['Magnitude'] * (1 + 70/df['Profondeur'])\n",
    "    \n",
    "    # Catégorisation du potentiel destructeur\n",
    "    potentiel_categories = [\n",
    "        (0, 3, 'Très faible'),\n",
    "        (3, 6, 'Faible'),\n",
    "        (6, 10, 'Modéré'),\n",
    "        (10, 15, 'Élevé'),\n",
    "        (15, float('inf'), 'Très élevé')\n",
    "    ]\n",
    "    \n",
    "    def categorize_potentiel(pot):\n",
    "        for low, high, category in potentiel_categories:\n",
    "            if low <= pot < high:\n",
    "                return category\n",
    "        return 'Inconnu'\n",
    "    \n",
    "    df['Potentiel_Categorie'] = df['Potentiel_Destructeur'].apply(categorize_potentiel)\n",
    "    \n",
    "    # Nettoyer les colonnes (supprimer les caractères \\r si présents)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.replace('\\r', '')\n",
    "    \n",
    "    print(f\"Données chargées: {len(df)} enregistrements\")\n",
    "    print(f\"Période couverte: {df['Date'].min().date()} à {df['Date'].max().date()}\")\n",
    "    return df\n",
    "    \n",
    "# Cette fonction peut aussi être modifiée séparément si vous ne voulez pas modifier toute la fonction de chargement\n",
    "def corriger_profondeurs_negatives(df):\n",
    "    \"\"\"Corrige les profondeurs négatives et recalcule le potentiel destructeur\"\"\"\n",
    "    if (df['Profondeur'] < 0).any():\n",
    "        print(f\"\\nAttention: {(df['Profondeur'] < 0).sum()} valeurs de profondeur négatives détectées.\")\n",
    "        print(\"Application de la valeur absolue pour ces valeurs...\")\n",
    "        \n",
    "        # Créer une copie du dataframe pour ne pas modifier l'original\n",
    "        df_corrige = df.copy()\n",
    "        \n",
    "        # Prendre la valeur absolue des profondeurs\n",
    "        df_corrige['Profondeur'] = df_corrige['Profondeur'].abs()\n",
    "        \n",
    "        # Recatégoriser les profondeurs\n",
    "        def categorize_depth(depth):\n",
    "            if 0 <= depth < 70:\n",
    "                return 'Peu profond'\n",
    "            elif 70 <= depth < 300:\n",
    "                return 'Intermédiaire'\n",
    "            elif depth >= 300:\n",
    "                return 'Profond'\n",
    "            return 'Inconnu'\n",
    "        \n",
    "        df_corrige['Profondeur_Categorie'] = df_corrige['Profondeur'].apply(categorize_depth)\n",
    "        \n",
    "        # Recalculer le potentiel destructeur\n",
    "        df_corrige['Potentiel_Destructeur'] = df_corrige['Magnitude'] * (1 + 70/df_corrige['Profondeur'])\n",
    "        \n",
    "        # Recatégoriser le potentiel destructeur\n",
    "        def categorize_potentiel(pot):\n",
    "            if 0 <= pot < 3:\n",
    "                return 'Très faible'\n",
    "            elif 3 <= pot < 6:\n",
    "                return 'Faible'\n",
    "            elif 6 <= pot < 10:\n",
    "                return 'Modéré'\n",
    "            elif 10 <= pot < 15:\n",
    "                return 'Élevé'\n",
    "            elif pot >= 15:\n",
    "                return 'Très élevé'\n",
    "            return 'Inconnu'\n",
    "        \n",
    "        df_corrige['Potentiel_Categorie'] = df_corrige['Potentiel_Destructeur'].apply(categorize_potentiel)\n",
    "        \n",
    "        return df_corrige\n",
    "    else:\n",
    "        print(\"Aucune profondeur négative détectée dans les données.\")\n",
    "        return df\n",
    "\n",
    "# Version corrigée de la fonction preparer_donnees_clustering\n",
    "def preparer_donnees_clustering(df):\n",
    "    \"\"\"Prépare les données pour le clustering avec gestion des valeurs extrêmes\"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"Préparation des données pour le clustering...\")\n",
    "    \n",
    "    # Sélectionner les caractéristiques pertinentes\n",
    "    features = ['Magnitude', 'Profondeur', 'Potentiel_Destructeur', 'Energie']\n",
    "    \n",
    "    # Créer le jeu de données pour le clustering\n",
    "    X = df[features].copy()\n",
    "    \n",
    "    # Vérifier et gérer les valeurs problématiques\n",
    "    for feature in features:\n",
    "        # Identifier les valeurs infinies ou NaN\n",
    "        has_inf = np.isinf(X[feature]).any()\n",
    "        has_nan = np.isnan(X[feature]).any()\n",
    "        has_huge = (np.abs(X[feature]) > 1e100).any()\n",
    "        \n",
    "        if has_inf or has_nan or has_huge:\n",
    "            print(f\"  Détecté dans '{feature}': infinies={has_inf}, NaN={has_nan}, valeurs extrêmes={has_huge}\")\n",
    "            \n",
    "            # Remplacer les valeurs infinies par la valeur max non-infinie * 2\n",
    "            if has_inf:\n",
    "                max_val = X[feature][~np.isinf(X[feature])].max()\n",
    "                min_val = X[feature][~np.isinf(X[feature])].min()\n",
    "                X.loc[X[feature] == np.inf, feature] = max_val * 2\n",
    "                X.loc[X[feature] == -np.inf, feature] = min_val * 2\n",
    "                print(f\"  Valeurs infinies dans '{feature}' remplacées par {max_val*2}\")\n",
    "            \n",
    "            # Remplacer les NaN par la médiane\n",
    "            if has_nan:\n",
    "                median = X[feature].median()\n",
    "                X[feature] = X[feature].fillna(median)\n",
    "                print(f\"  NaN dans '{feature}' remplacés par la médiane: {median}\")\n",
    "            \n",
    "            # Plafonner les valeurs extrêmes\n",
    "            if has_huge:\n",
    "                upper_limit = np.percentile(X[feature][~np.isinf(X[feature]) & ~np.isnan(X[feature])], 99)\n",
    "                lower_limit = np.percentile(X[feature][~np.isinf(X[feature]) & ~np.isnan(X[feature])], 1)\n",
    "                X.loc[X[feature] > upper_limit, feature] = upper_limit\n",
    "                X.loc[X[feature] < lower_limit, feature] = lower_limit\n",
    "                print(f\"  Valeurs extrêmes dans '{feature}' plafonnées entre {lower_limit} et {upper_limit}\")\n",
    "    \n",
    "    # Pour l'énergie qui peut avoir des valeurs très différentes, utiliser le logarithme\n",
    "    if 'Energie' in features:\n",
    "        # Garantir que l'énergie est positive\n",
    "        min_energy = X['Energie'][X['Energie'] > 0].min()\n",
    "        X.loc[X['Energie'] <= 0, 'Energie'] = min_energy\n",
    "        # Appliquer la transformation logarithmique\n",
    "        X['Energie'] = np.log10(X['Energie'])\n",
    "        print(\"  Transformation logarithmique appliquée à l'énergie\")\n",
    "    \n",
    "    # Traiter spécifiquement le potentiel destructeur\n",
    "    if 'Potentiel_Destructeur' in features:\n",
    "        # En cas de valeurs négatives, décaler pour que tout soit positif\n",
    "        min_pd = X['Potentiel_Destructeur'].min()\n",
    "        if min_pd < 0:\n",
    "            X['Potentiel_Destructeur'] = X['Potentiel_Destructeur'] - min_pd + 1\n",
    "            print(f\"  Potentiel destructeur décalé pour éliminer les valeurs négatives (min = {min_pd})\")\n",
    "    \n",
    "    print(\"Vérification finale des données:\")\n",
    "    for feature in features:\n",
    "        if np.isinf(X[feature]).any() or np.isnan(X[feature]).any():\n",
    "            print(f\"  ATTENTION: {feature} contient encore des valeurs problématiques après nettoyage!\")\n",
    "    \n",
    "    # Utiliser RobustScaler qui est moins sensible aux valeurs aberrantes\n",
    "    print(\"Application de la mise à l'échelle robuste...\")\n",
    "    scaler = RobustScaler()\n",
    "    try:\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        print(\"Mise à l'échelle réussie!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la mise à l'échelle: {e}\")\n",
    "        # Plan B: Mise à l'échelle manuelle\n",
    "        print(\"Tentative de mise à l'échelle manuelle...\")\n",
    "        X_scaled = np.zeros_like(X.values, dtype=float)\n",
    "        for i, feature in enumerate(features):\n",
    "            median = X[feature].median()\n",
    "            iqr = X[feature].quantile(0.75) - X[feature].quantile(0.25)\n",
    "            iqr = max(iqr, 1e-10)  # Éviter la division par zéro\n",
    "            X_scaled[:, i] = (X[feature].values - median) / iqr\n",
    "        print(\"Mise à l'échelle manuelle réussie!\")\n",
    "    \n",
    "    return X_scaled, features\n",
    "\n",
    "# Modification optionnelle pour la fonction analyser_clustering\n",
    "def analyser_clustering(df_filtered):\n",
    "    \"\"\"Analyse les données sismiques par clustering avec gestion d'erreurs améliorée\"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.cluster import KMeans\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\\nAnalyse par clustering des données sismiques...\")\n",
    "    \n",
    "    # Vérifier que nous avons assez de données\n",
    "    if len(df_filtered) < 10:\n",
    "        print(\"Pas assez de données pour le clustering (minimum 10 points requis).\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Préparation des données\n",
    "        X_scaled, features = preparer_donnees_clustering(df_filtered)\n",
    "        \n",
    "        # Vérifier qu'il n'y a pas de valeurs problématiques restantes\n",
    "        if np.isnan(X_scaled).any() or np.isinf(X_scaled).any():\n",
    "            print(\"ERREUR: Des valeurs infinies ou NaN persistent après prétraitement.\")\n",
    "            # Dernière tentative de nettoyage\n",
    "            X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=10.0, neginf=-10.0)\n",
    "            print(\"Remplacement forcé des valeurs problématiques.\")\n",
    "        \n",
    "        # Déterminer le nombre optimal de clusters\n",
    "        try:\n",
    "            optimal_k = determiner_nombre_clusters(X_scaled)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la détermination du nombre optimal de clusters: {e}\")\n",
    "            print(\"Utilisation de 3 clusters par défaut.\")\n",
    "            optimal_k = 3\n",
    "        \n",
    "        # Réaliser le clustering et analyser les résultats\n",
    "        try:\n",
    "            df_clusters = realiser_clustering(X_scaled, optimal_k, df_filtered, features)\n",
    "            \n",
    "            # Interprétation des clusters\n",
    "            print(\"\\nInterprétation suggérée des clusters:\")\n",
    "            \n",
    "            # Trouver les caractéristiques distinctives de chaque cluster\n",
    "            for cluster in range(optimal_k):\n",
    "                cluster_data = df_clusters[df_clusters['Cluster'] == cluster]\n",
    "                mean_mag = cluster_data['Magnitude'].mean()\n",
    "                mean_depth = cluster_data['Profondeur'].mean()\n",
    "                mean_pd = cluster_data['Potentiel_Destructeur'].mean()\n",
    "                \n",
    "                print(f\"\\nCluster {cluster} ({len(cluster_data)} séismes):\")\n",
    "                print(f\"  Magnitude moyenne: {mean_mag:.2f}\")\n",
    "                print(f\"  Profondeur moyenne: {mean_depth:.2f} km\")\n",
    "                print(f\"  Potentiel destructeur moyen: {mean_pd:.2f}\")\n",
    "                \n",
    "                # Caractérisation du cluster\n",
    "                if mean_mag < 2.0 and mean_depth < 50:\n",
    "                    print(\"  → Probablement des micro-séismes superficiels\")\n",
    "                elif mean_mag < 2.0 and mean_depth >= 50:\n",
    "                    print(\"  → Probablement des micro-séismes profonds\")\n",
    "                elif mean_mag >= 2.0 and mean_mag < 4.0 and mean_depth < 50:\n",
    "                    print(\"  → Probablement des séismes volcano-tectoniques superficiels\")\n",
    "                elif mean_mag >= 2.0 and mean_mag < 4.0 and mean_depth >= 50:\n",
    "                    print(\"  → Probablement des séismes tectoniques régionaux\")\n",
    "                elif mean_mag >= 4.0:\n",
    "                    print(\"  → Probablement des événements majeurs\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la réalisation du clustering: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur générale lors de l'analyse par clustering: {e}\")\n",
    "        print(\"Assurez-vous que vos données ne contiennent pas de valeurs aberrantes extrêmes.\")\n",
    "\n",
    "def determiner_nombre_clusters(X_scaled):\n",
    "    \"\"\"Détermine le nombre optimal de clusters avec gestion d'erreurs améliorée\"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Définir la plage de nombre de clusters à tester\n",
    "    k_range = range(2, 11)  # De 2 à 10 clusters\n",
    "    \n",
    "    # Initialiser les listes pour stocker les mesures\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    print(\"Calcul des mesures pour différents nombres de clusters...\")\n",
    "    \n",
    "    # Calculer l'inertie et le score de silhouette pour chaque valeur de k\n",
    "    for k in k_range:\n",
    "        print(f\"  Évaluation pour k = {k}...\")\n",
    "        \n",
    "        # Entraîner le modèle KMeans\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        \n",
    "        # Stocker l'inertie\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        \n",
    "        # Calculer et stocker le score de silhouette (seulement pour k > 1)\n",
    "        if k > 1:\n",
    "            try:\n",
    "                labels = kmeans.labels_\n",
    "                score = silhouette_score(X_scaled, labels)\n",
    "                silhouette_scores.append(score)\n",
    "                print(f\"    Score de silhouette: {score:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Erreur lors du calcul du score de silhouette pour k={k}: {e}\")\n",
    "                silhouette_scores.append(0)  # Valeur par défaut en cas d'erreur\n",
    "    \n",
    "    # Tracer la courbe du coude et les scores de silhouette\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Courbe du coude\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(k_range), inertias, 'o-', color='blue')\n",
    "    plt.xlabel('Nombre de clusters (k)')\n",
    "    plt.ylabel('Inertie')\n",
    "    plt.title('Méthode du coude')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Scores de silhouette\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # CORRECTION ICI : k_range commence à 2, mais silhouette_scores commence à partir de k=2\n",
    "    # Nous devons donc utiliser k_range[2-2:] pour aligner les dimensions\n",
    "    plt.plot(list(k_range)[2-2:], silhouette_scores, 'o-', color='green')\n",
    "    plt.xlabel('Nombre de clusters (k)')\n",
    "    plt.ylabel('Score de silhouette')\n",
    "    plt.title('Score de silhouette')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Déterminer le nombre optimal de clusters\n",
    "    if silhouette_scores:\n",
    "        best_silhouette_idx = np.argmax(silhouette_scores)\n",
    "        # Pour obtenir la valeur k correspondante, nous devons ajouter 2 à l'index\n",
    "        # car silhouette_scores commence à k=2\n",
    "        optimal_k = list(k_range)[best_silhouette_idx + (2-2)]\n",
    "        print(f\"Nombre optimal suggéré de clusters selon le score de silhouette: {optimal_k}\")\n",
    "    else:\n",
    "        # Valeur par défaut si aucun score de silhouette n'a pu être calculé\n",
    "        optimal_k = 3\n",
    "        print(f\"Impossible de déterminer le nombre optimal. Utilisation de la valeur par défaut: {optimal_k}\")\n",
    "    \n",
    "    return optimal_k\n",
    "\n",
    "def realiser_clustering(X_scaled, n_clusters, df, features):\n",
    "    \"\"\"Réalise le clustering et analyse les résultats\"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Appliquer K-means avec le nombre optimal de clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df_clusters = df.copy()\n",
    "    df_clusters['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Analyser les caractéristiques de chaque cluster\n",
    "    cluster_stats = df_clusters.groupby('Cluster').agg({\n",
    "        'Magnitude': ['mean', 'min', 'max', 'count'],\n",
    "        'Profondeur': ['mean', 'min', 'max'],\n",
    "        'Potentiel_Destructeur': ['mean', 'min', 'max'],\n",
    "        'Energie': ['mean', 'sum']\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nCaractéristiques des clusters identifiés:\")\n",
    "    display(cluster_stats)\n",
    "    \n",
    "    # Visualisation des clusters (en 2D en utilisant les 2 caractéristiques les plus importantes)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Visualiser les clusters pour différentes paires de caractéristiques\n",
    "    feature_pairs = [\n",
    "        ('Magnitude', 'Profondeur'),\n",
    "        ('Magnitude', 'Potentiel_Destructeur'),\n",
    "        ('Profondeur', 'Potentiel_Destructeur')\n",
    "    ]\n",
    "    \n",
    "    for i, (feature1, feature2) in enumerate(feature_pairs):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        \n",
    "        # Créer des couleurs distinctes pour chaque cluster\n",
    "        scatter = plt.scatter(\n",
    "            df_clusters[feature1], \n",
    "            df_clusters[feature2], \n",
    "            c=df_clusters['Cluster'], \n",
    "            cmap='viridis', \n",
    "            alpha=0.6,\n",
    "            s=50\n",
    "        )\n",
    "        \n",
    "        # Ajouter les centroïdes\n",
    "        centers = kmeans.cluster_centers_\n",
    "        feature1_idx = features.index(feature1)\n",
    "        feature2_idx = features.index(feature2)\n",
    "        plt.scatter(\n",
    "            centers[:, feature1_idx], \n",
    "            centers[:, feature2_idx], \n",
    "            c='red', \n",
    "            marker='X', \n",
    "            s=200, \n",
    "            label='Centroïdes'\n",
    "        )\n",
    "        \n",
    "        plt.xlabel(feature1)\n",
    "        plt.ylabel(feature2)\n",
    "        plt.title(f'Clusters: {feature1} vs {feature2}')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Distribution des clusters dans le temps (si les données temporelles sont disponibles)\n",
    "    if 'Date' in df_clusters.columns:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        \n",
    "        # Compter les séismes par cluster et par mois\n",
    "        df_clusters['YearMonth'] = df_clusters['Date'].dt.to_period('M')\n",
    "        time_clusters = df_clusters.groupby(['YearMonth', 'Cluster']).size().unstack().fillna(0)\n",
    "        \n",
    "        # Tracer la distribution temporelle\n",
    "        time_clusters.plot(kind='area', stacked=True, colormap='viridis', alpha=0.7, ax=plt.gca())\n",
    "        plt.xlabel('Mois')\n",
    "        plt.ylabel('Nombre de séismes')\n",
    "        plt.title('Évolution temporelle des clusters')\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Option: visualisation 3D\n",
    "    try:\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        scatter = ax.scatter(\n",
    "            df_clusters['Magnitude'],\n",
    "            df_clusters['Profondeur'],\n",
    "            df_clusters['Potentiel_Destructeur'],\n",
    "            c=df_clusters['Cluster'],\n",
    "            cmap='viridis',\n",
    "            s=50,\n",
    "            alpha=0.6\n",
    "        )\n",
    "        \n",
    "        ax.set_xlabel('Magnitude')\n",
    "        ax.set_ylabel('Profondeur')\n",
    "        ax.set_zlabel('Potentiel Destructeur')\n",
    "        ax.set_title('Clusters sismiques en 3D')\n",
    "        \n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(\"Visualisation 3D non disponible\")\n",
    "    \n",
    "    return df_clusters\n",
    "\n",
    "# Nouvelle fonction d'analyse par clustering à ajouter à votre code\n",
    "def analyser_clustering(df_filtered):\n",
    "    \"\"\"Analyse les données sismiques par clustering\"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    print(\"\\nAnalyse par clustering des données sismiques...\")\n",
    "    \n",
    "    # Préparation des données\n",
    "    X_scaled, features = preparer_donnees_clustering(df_filtered)\n",
    "    \n",
    "    # Déterminer le nombre optimal de clusters\n",
    "    optimal_k = determiner_nombre_clusters(X_scaled)\n",
    "    \n",
    "    # Réaliser le clustering et analyser les résultats\n",
    "    df_clusters = realiser_clustering(X_scaled, optimal_k, df_filtered, features)\n",
    "    \n",
    "    # Interprétation des clusters\n",
    "    print(\"\\nInterprétation suggérée des clusters:\")\n",
    "    \n",
    "    # Trouver les caractéristiques distinctives de chaque cluster\n",
    "    for cluster in range(optimal_k):\n",
    "        cluster_data = df_clusters[df_clusters['Cluster'] == cluster]\n",
    "        mean_mag = cluster_data['Magnitude'].mean()\n",
    "        mean_depth = cluster_data['Profondeur'].mean()\n",
    "        mean_pd = cluster_data['Potentiel_Destructeur'].mean()\n",
    "        \n",
    "        print(f\"\\nCluster {cluster} ({len(cluster_data)} séismes):\")\n",
    "        print(f\"  Magnitude moyenne: {mean_mag:.2f}\")\n",
    "        print(f\"  Profondeur moyenne: {mean_depth:.2f} km\")\n",
    "        print(f\"  Potentiel destructeur moyen: {mean_pd:.2f}\")\n",
    "        \n",
    "        # Caractérisation du cluster\n",
    "        if mean_mag < 2.0 and mean_depth < 50:\n",
    "            print(\"  → Probablement des micro-séismes superficiels\")\n",
    "        elif mean_mag < 2.0 and mean_depth >= 50:\n",
    "            print(\"  → Probablement des micro-séismes profonds\")\n",
    "        elif mean_mag >= 2.0 and mean_mag < 4.0 and mean_depth < 50:\n",
    "            print(\"  → Probablement des séismes volcano-tectoniques superficiels\")\n",
    "        elif mean_mag >= 2.0 and mean_mag < 4.0 and mean_depth >= 50:\n",
    "            print(\"  → Probablement des séismes tectoniques régionaux\")\n",
    "        elif mean_mag >= 4.0:\n",
    "            print(\"  → Probablement des événements majeurs\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fonction pour appliquer les filtres aux données\n",
    "def appliquer_filtres(df, annee_range, mois_selected, magnitude_range, profondeur_range, \n",
    "                     potentiel_categories=None, magnitude_categories=None, profondeur_categories=None):\n",
    "    \"\"\"Applique les filtres sélectionnés au dataframe\"\"\"\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    # Filtrer par année\n",
    "    df_filtered = df_filtered[(df_filtered['Annee'] >= annee_range[0]) & \n",
    "                            (df_filtered['Annee'] <= annee_range[1])]\n",
    "    \n",
    "    # Filtrer par mois\n",
    "    df_filtered = df_filtered[df_filtered['Mois'].isin(mois_selected)]\n",
    "    \n",
    "    # Filtrer par magnitude\n",
    "    df_filtered = df_filtered[(df_filtered['Magnitude'] >= magnitude_range[0]) & \n",
    "                            (df_filtered['Magnitude'] <= magnitude_range[1])]\n",
    "    \n",
    "    # Filtrer par profondeur\n",
    "    df_filtered = df_filtered[(df_filtered['Profondeur'] >= profondeur_range[0]) & \n",
    "                            (df_filtered['Profondeur'] <= profondeur_range[1])]\n",
    "    \n",
    "    # Filtrer par catégorie de potentiel destructeur\n",
    "    if potentiel_categories:\n",
    "        df_filtered = df_filtered[df_filtered['Potentiel_Categorie'].isin(potentiel_categories)]\n",
    "    \n",
    "    # Filtrer par catégorie de magnitude\n",
    "    if magnitude_categories:\n",
    "        df_filtered = df_filtered[df_filtered['Magnitude_Categorie'].isin(magnitude_categories)]\n",
    "    \n",
    "    # Filtrer par catégorie de profondeur\n",
    "    if profondeur_categories:\n",
    "        df_filtered = df_filtered[df_filtered['Profondeur_Categorie'].isin(profondeur_categories)]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# 1. Analyse de la distribution des magnitudes\n",
    "def analyser_distribution_magnitudes(df_filtered):\n",
    "    \"\"\"Analyse la distribution des magnitudes des séismes\"\"\"\n",
    "    \n",
    "    # 1.1 Distribution globale des magnitudes\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.histplot(df_filtered['Magnitude'], bins=30, kde=True)\n",
    "    plt.title('Distribution des magnitudes des séismes')\n",
    "    plt.xlabel('Magnitude')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.axvline(x=df_filtered['Magnitude'].mean(), color='r', linestyle='--', \n",
    "               label=f'Moyenne: {df_filtered[\"Magnitude\"].mean():.2f}')\n",
    "    plt.axvline(x=df_filtered['Magnitude'].median(), color='g', linestyle='--', \n",
    "               label=f'Médiane: {df_filtered[\"Magnitude\"].median():.2f}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 1.2 Distribution par catégorie de magnitude\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    # Obtenir les catégories dans l'ordre croissant de magnitude\n",
    "    order = ['Micro', 'Faible', 'Léger', 'Modéré', 'Fort', 'Majeur', 'Grand']\n",
    "    # Filtrer pour n'inclure que les catégories présentes dans les données\n",
    "    order = [cat for cat in order if cat in df_filtered['Magnitude_Categorie'].unique()]\n",
    "    \n",
    "    mag_counts = df_filtered['Magnitude_Categorie'].value_counts().reindex(order)\n",
    "    \n",
    "    # Créer un dégradé de couleurs bleu à rouge\n",
    "    palette = sns.color_palette(\"YlOrRd\", len(order))\n",
    "    \n",
    "    ax = sns.barplot(x=mag_counts.index, y=mag_counts.values, palette=palette)\n",
    "    plt.title('Nombre de séismes par catégorie de magnitude')\n",
    "    plt.xlabel('Catégorie de magnitude')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    \n",
    "    # Ajouter les pourcentages sur chaque barre\n",
    "    total = len(df_filtered)\n",
    "    for i, count in enumerate(mag_counts.values):\n",
    "        percentage = count / total * 100\n",
    "        ax.text(i, count + 5, f'{percentage:.1f}%', ha='center')\n",
    "    \n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher les statistiques de magnitude\n",
    "    print(\"\\nStatistiques des magnitudes:\")\n",
    "    print(f\"Nombre total de séismes: {len(df_filtered)}\")\n",
    "    print(f\"Magnitude minimale: {df_filtered['Magnitude'].min():.2f}\")\n",
    "    print(f\"Magnitude maximale: {df_filtered['Magnitude'].max():.2f}\")\n",
    "    print(f\"Magnitude moyenne: {df_filtered['Magnitude'].mean():.2f}\")\n",
    "    print(f\"Magnitude médiane: {df_filtered['Magnitude'].median():.2f}\")\n",
    "    print(f\"Écart-type des magnitudes: {df_filtered['Magnitude'].std():.2f}\")\n",
    "    \n",
    "    print(\"\\nRépartition par catégorie de magnitude:\")\n",
    "    percentage_by_category = df_filtered['Magnitude_Categorie'].value_counts(normalize=True) * 100\n",
    "    counts_by_category = df_filtered['Magnitude_Categorie'].value_counts()\n",
    "    \n",
    "    for category in order:\n",
    "        if category in percentage_by_category:\n",
    "            print(f\"{category}: {counts_by_category[category]} séismes ({percentage_by_category[category]:.1f}%)\")\n",
    "\n",
    "# 2. Analyse de la distribution des profondeurs\n",
    "def analyser_distribution_profondeurs(df_filtered):\n",
    "    \"\"\"Analyse la distribution des profondeurs des séismes\"\"\"\n",
    "    \n",
    "    # 2.1 Distribution globale des profondeurs\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.histplot(df_filtered['Profondeur'], bins=30, kde=True)\n",
    "    plt.title('Distribution des profondeurs des séismes')\n",
    "    plt.xlabel('Profondeur (km)')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.axvline(x=df_filtered['Profondeur'].mean(), color='r', linestyle='--', \n",
    "               label=f'Moyenne: {df_filtered[\"Profondeur\"].mean():.2f} km')\n",
    "    plt.axvline(x=df_filtered['Profondeur'].median(), color='g', linestyle='--', \n",
    "               label=f'Médiane: {df_filtered[\"Profondeur\"].median():.2f} km')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2.2 Distribution par catégorie de profondeur\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    # Obtenir les catégories dans l'ordre croissant de profondeur\n",
    "    order = ['Peu profond', 'Intermédiaire', 'Profond']\n",
    "    # Filtrer pour n'inclure que les catégories présentes dans les données\n",
    "    order = [cat for cat in order if cat in df_filtered['Profondeur_Categorie'].unique()]\n",
    "    \n",
    "    depth_counts = df_filtered['Profondeur_Categorie'].value_counts().reindex(order)\n",
    "    \n",
    "    # Créer un dégradé de couleurs bleu\n",
    "    palette = sns.color_palette(\"Blues\", len(order))\n",
    "    \n",
    "    ax = sns.barplot(x=depth_counts.index, y=depth_counts.values, palette=palette)\n",
    "    plt.title('Nombre de séismes par catégorie de profondeur')\n",
    "    plt.xlabel('Catégorie de profondeur')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    \n",
    "    # Ajouter les pourcentages sur chaque barre\n",
    "    total = len(df_filtered)\n",
    "    for i, count in enumerate(depth_counts.values):\n",
    "        percentage = count / total * 100\n",
    "        ax.text(i, count + 5, f'{percentage:.1f}%', ha='center')\n",
    "    \n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher les statistiques de profondeur\n",
    "    print(\"\\nStatistiques des profondeurs:\")\n",
    "    print(f\"Profondeur minimale: {df_filtered['Profondeur'].min():.2f} km\")\n",
    "    print(f\"Profondeur maximale: {df_filtered['Profondeur'].max():.2f} km\")\n",
    "    print(f\"Profondeur moyenne: {df_filtered['Profondeur'].mean():.2f} km\")\n",
    "    print(f\"Profondeur médiane: {df_filtered['Profondeur'].median():.2f} km\")\n",
    "    print(f\"Écart-type des profondeurs: {df_filtered['Profondeur'].std():.2f} km\")\n",
    "    \n",
    "    print(\"\\nRépartition par catégorie de profondeur:\")\n",
    "    percentage_by_category = df_filtered['Profondeur_Categorie'].value_counts(normalize=True) * 100\n",
    "    counts_by_category = df_filtered['Profondeur_Categorie'].value_counts()\n",
    "    \n",
    "    for category in order:\n",
    "        if category in percentage_by_category:\n",
    "            print(f\"{category}: {counts_by_category[category]} séismes ({percentage_by_category[category]:.1f}%)\")\n",
    "\n",
    "# 3. Analyse de la relation entre magnitude et profondeur\n",
    "def analyser_relation_magnitude_profondeur(df_filtered):\n",
    "    \"\"\"Analyse la relation entre magnitude et profondeur des séismes\"\"\"\n",
    "    \n",
    "    # 3.1 Nuage de points magnitude vs profondeur\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Utiliser une palette de couleurs pour le potentiel destructeur\n",
    "    scatter = plt.scatter(df_filtered['Profondeur'], df_filtered['Magnitude'], \n",
    "                         c=df_filtered['Potentiel_Destructeur'], cmap='YlOrRd', \n",
    "                         alpha=0.7, s=50)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Potentiel destructeur')\n",
    "    plt.title('Relation entre magnitude et profondeur des séismes')\n",
    "    plt.xlabel('Profondeur (km)')\n",
    "    plt.ylabel('Magnitude')\n",
    "    \n",
    "    # Ajouter une courbe de régression\n",
    "    if len(df_filtered) > 2:\n",
    "        try:\n",
    "            # Régression linéaire simple\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "                df_filtered['Profondeur'], df_filtered['Magnitude'])\n",
    "            \n",
    "            x = np.array([df_filtered['Profondeur'].min(), df_filtered['Profondeur'].max()])\n",
    "            y = intercept + slope * x\n",
    "            \n",
    "            plt.plot(x, y, 'b--', \n",
    "                   label=f'Régression: y={slope:.4f}x+{intercept:.4f}, r²={r_value**2:.3f}')\n",
    "            plt.legend()\n",
    "            \n",
    "            print(f\"\\nAnalyse de corrélation entre magnitude et profondeur:\")\n",
    "            print(f\"Coefficient de corrélation (r): {r_value:.3f}\")\n",
    "            print(f\"Coefficient de détermination (r²): {r_value**2:.3f}\")\n",
    "            print(f\"p-value: {p_value:.4f}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                print(\"Il existe une relation statistiquement significative entre la magnitude et la profondeur.\")\n",
    "                if slope > 0:\n",
    "                    print(\"La magnitude tend à augmenter avec la profondeur.\")\n",
    "                else:\n",
    "                    print(\"La magnitude tend à diminuer avec la profondeur.\")\n",
    "            else:\n",
    "                print(\"Aucune relation statistiquement significative n'a été détectée entre la magnitude et la profondeur.\")\n",
    "        except:\n",
    "            print(\"Impossible de calculer la régression avec les données actuelles.\")\n",
    "    \n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3.2 Heatmap magnitude vs profondeur\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Créer des bins pour la magnitude et la profondeur\n",
    "    mag_bins = np.linspace(df_filtered['Magnitude'].min(), df_filtered['Magnitude'].max(), 15)\n",
    "    depth_bins = np.linspace(df_filtered['Profondeur'].min(), df_filtered['Profondeur'].max(), 15)\n",
    "    \n",
    "    # Créer un histogramme 2D\n",
    "    heatmap, xedges, yedges = np.histogram2d(df_filtered['Profondeur'], df_filtered['Magnitude'], \n",
    "                                           bins=[depth_bins, mag_bins])\n",
    "    \n",
    "    # Afficher la heatmap\n",
    "    plt.pcolormesh(xedges, yedges, heatmap.T, cmap='YlOrRd', shading='auto')\n",
    "    plt.colorbar(label='Nombre de séismes')\n",
    "    plt.title('Heatmap de la relation entre magnitude et profondeur')\n",
    "    plt.xlabel('Profondeur (km)')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3.3 Box plots de magnitude par catégorie de profondeur\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    order = ['Peu profond', 'Intermédiaire', 'Profond']\n",
    "    order = [cat for cat in order if cat in df_filtered['Profondeur_Categorie'].unique()]\n",
    "    \n",
    "    sns.boxplot(x='Profondeur_Categorie', y='Magnitude', data=df_filtered, order=order, palette='Blues')\n",
    "    plt.title('Distribution des magnitudes par catégorie de profondeur')\n",
    "    plt.xlabel('Catégorie de profondeur')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques par catégorie de profondeur\n",
    "    print(\"\\nStatistiques de magnitude par catégorie de profondeur:\")\n",
    "    for category in order:\n",
    "        if category in df_filtered['Profondeur_Categorie'].unique():\n",
    "            mag_stats = df_filtered[df_filtered['Profondeur_Categorie'] == category]['Magnitude'].describe()\n",
    "            print(f\"\\n{category}:\")\n",
    "            print(f\"  Nombre de séismes: {mag_stats['count']:.0f}\")\n",
    "            print(f\"  Magnitude moyenne: {mag_stats['mean']:.2f}\")\n",
    "            print(f\"  Magnitude médiane: {mag_stats['50%']:.2f}\")\n",
    "            print(f\"  Magnitude maximale: {mag_stats['max']:.2f}\")\n",
    "            print(f\"  Écart-type: {mag_stats['std']:.2f}\")\n",
    "\n",
    "# 4. Analyse du potentiel destructeur\n",
    "def analyser_potentiel_destructeur(df_filtered):\n",
    "    \"\"\"Analyse le potentiel destructeur des séismes (combinaison de magnitude et profondeur)\"\"\"\n",
    "    \n",
    "    # 4.1 Distribution du potentiel destructeur\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.histplot(df_filtered['Potentiel_Destructeur'], bins=30, kde=True)\n",
    "    plt.title('Distribution du potentiel destructeur des séismes')\n",
    "    plt.xlabel('Potentiel destructeur')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.axvline(x=df_filtered['Potentiel_Destructeur'].mean(), color='r', linestyle='--', \n",
    "               label=f'Moyenne: {df_filtered[\"Potentiel_Destructeur\"].mean():.2f}')\n",
    "    plt.axvline(x=df_filtered['Potentiel_Destructeur'].median(), color='g', linestyle='--', \n",
    "               label=f'Médiane: {df_filtered[\"Potentiel_Destructeur\"].median():.2f}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4.2 Distribution par catégorie de potentiel destructeur\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    # Obtenir les catégories dans l'ordre croissant de potentiel\n",
    "    order = ['Très faible', 'Faible', 'Modéré', 'Élevé', 'Très élevé']\n",
    "    # Filtrer pour n'inclure que les catégories présentes dans les données\n",
    "    order = [cat for cat in order if cat in df_filtered['Potentiel_Categorie'].unique()]\n",
    "    \n",
    "    potentiel_counts = df_filtered['Potentiel_Categorie'].value_counts().reindex(order)\n",
    "    \n",
    "    # Créer un dégradé de couleurs\n",
    "    palette = sns.color_palette(\"YlOrRd\", len(order))\n",
    "    \n",
    "    ax = sns.barplot(x=potentiel_counts.index, y=potentiel_counts.values, palette=palette)\n",
    "    plt.title('Nombre de séismes par catégorie de potentiel destructeur')\n",
    "    plt.xlabel('Catégorie de potentiel destructeur')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    \n",
    "    # Ajouter les pourcentages sur chaque barre\n",
    "    total = len(df_filtered)\n",
    "    for i, count in enumerate(potentiel_counts.values):\n",
    "        percentage = count / total * 100\n",
    "        ax.text(i, count + 5, f'{percentage:.1f}%', ha='center')\n",
    "    \n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4.3 Carte de chaleur du potentiel destructeur (magnitude vs profondeur)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Créer une grille pour la carte de chaleur\n",
    "    x = np.linspace(df_filtered['Profondeur'].min(), df_filtered['Profondeur'].max(), 100)\n",
    "    y = np.linspace(df_filtered['Magnitude'].min(), df_filtered['Magnitude'].max(), 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = Y * (1 + 70/X)  # Formule du potentiel destructeur\n",
    "    \n",
    "    # Tracer la carte de chaleur\n",
    "    contour = plt.contourf(X, Y, Z, 20, cmap='YlOrRd')\n",
    "    plt.colorbar(contour, label='Potentiel destructeur')\n",
    "    \n",
    "    # Superposer les points des séismes réels\n",
    "    plt.scatter(df_filtered['Profondeur'], df_filtered['Magnitude'], \n",
    "               c='black', alpha=0.5, s=20)\n",
    "    \n",
    "    plt.title('Carte du potentiel destructeur (magnitude vs profondeur)')\n",
    "    plt.xlabel('Profondeur (km)')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques du potentiel destructeur\n",
    "    print(\"\\nStatistiques du potentiel destructeur:\")\n",
    "    print(f\"Potentiel destructeur minimum: {df_filtered['Potentiel_Destructeur'].min():.2f}\")\n",
    "    print(f\"Potentiel destructeur maximum: {df_filtered['Potentiel_Destructeur'].max():.2f}\")\n",
    "    print(f\"Potentiel destructeur moyen: {df_filtered['Potentiel_Destructeur'].mean():.2f}\")\n",
    "    print(f\"Potentiel destructeur médian: {df_filtered['Potentiel_Destructeur'].median():.2f}\")\n",
    "    print(f\"Écart-type du potentiel destructeur: {df_filtered['Potentiel_Destructeur'].std():.2f}\")\n",
    "    \n",
    "    print(\"\\nRépartition par catégorie de potentiel destructeur:\")\n",
    "    percentage_by_category = df_filtered['Potentiel_Categorie'].value_counts(normalize=True) * 100\n",
    "    counts_by_category = df_filtered['Potentiel_Categorie'].value_counts()\n",
    "    \n",
    "    for category in order:\n",
    "        if category in percentage_by_category:\n",
    "            print(f\"{category}: {counts_by_category[category]} séismes ({percentage_by_category[category]:.1f}%)\")\n",
    "\n",
    "# 5. Analyse de l'énergie libérée\n",
    "def analyser_energie(df_filtered):\n",
    "    \"\"\"Analyse l'énergie libérée par les séismes\"\"\"\n",
    "    \n",
    "    # 5.1 Distribution de l'énergie (échelle logarithmique)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.histplot(np.log10(df_filtered['Energie']), bins=30, kde=True)\n",
    "    plt.title('Distribution de l\\'énergie libérée par les séismes (échelle logarithmique)')\n",
    "    plt.xlabel('Énergie (log10 Joules)')\n",
    "    plt.ylabel('Nombre de séismes')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5.2 Énergie cumulée dans le temps\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Trier les données par date\n",
    "    df_sorted = df_filtered.sort_values('Date')\n",
    "    \n",
    "    # Calculer l'énergie cumulée\n",
    "    energie_cumulee = df_sorted['Energie'].cumsum()\n",
    "    \n",
    "    plt.plot(df_sorted['Date'], energie_cumulee)\n",
    "    plt.title('Énergie sismique cumulée au fil du temps')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Énergie cumulée (Joules)')\n",
    "    plt.yscale('log')  # Échelle logarithmique pour mieux visualiser\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5.3 Comparaison de l'énergie par catégorie de magnitude\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Obtenir les catégories dans l'ordre croissant\n",
    "    order = ['Micro', 'Faible', 'Léger', 'Modéré', 'Fort', 'Majeur', 'Grand']\n",
    "    order = [cat for cat in order if cat in df_filtered['Magnitude_Categorie'].unique()]\n",
    "    \n",
    "    # Calculer l'énergie totale par catégorie\n",
    "    energie_par_categorie = df_filtered.groupby('Magnitude_Categorie')['Energie'].sum()\n",
    "    energie_par_categorie = energie_par_categorie.reindex(order)\n",
    "    \n",
    "    # Création d'un dégradé de couleurs\n",
    "    palette = sns.color_palette(\"YlOrRd\", len(order))\n",
    "    \n",
    "    # Graphique en échelle logarithmique\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    ax = sns.barplot(x=energie_par_categorie.index, y=energie_par_categorie.values, palette=palette)\n",
    "    plt.title('Énergie totale libérée par catégorie de magnitude')\n",
    "    plt.xlabel('Catégorie de magnitude')\n",
    "    plt.ylabel('Énergie totale (Joules)')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Ajouter les pourcentages sur chaque barre\n",
    "    total = energie_par_categorie.sum()\n",
    "    for i, energy in enumerate(energie_par_categorie.values):\n",
    "        percentage = energy / total * 100\n",
    "        ax.text(i, energy * 1.1, f'{percentage:.1f}%', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques d'énergie\n",
    "    print(\"\\nStatistiques de l'énergie libérée:\")\n",
    "    print(f\"Énergie totale libérée: {df_filtered['Energie'].sum():.2e} Joules\")\n",
    "    print(f\"Énergie moyenne par séisme: {df_filtered['Energie'].mean():.2e} Joules\")\n",
    "    print(f\"Énergie médiane par séisme: {df_filtered['Energie'].median():.2e} Joules\")\n",
    "    print(f\"Séisme le plus énergétique: {df_filtered['Energie'].max():.2e} Joules\")\n",
    "    \n",
    "    print(\"\\nRépartition de l'énergie par catégorie de magnitude:\")\n",
    "    total_energy = df_filtered['Energie'].sum()\n",
    "    \n",
    "    for category in order:\n",
    "        if category in df_filtered['Magnitude_Categorie'].unique():\n",
    "            category_energy = df_filtered[df_filtered['Magnitude_Categorie'] == category]['Energie'].sum()\n",
    "            percentage = category_energy / total_energy * 100\n",
    "            count = len(df_filtered[df_filtered['Magnitude_Categorie'] == category])\n",
    "            print(f\"{category}: {category_energy:.2e} Joules ({percentage:.1f}% de l'énergie totale, {count} séismes)\")\n",
    "\n",
    "# Création du Dashboard\n",
    "# Correction pour la fonction creer_dashboard\n",
    "def creer_dashboard():\n",
    "    # Charger les données\n",
    "    df = charger_donnees()\n",
    "    \n",
    "    # Créer les widgets pour les filtres\n",
    "    output_dashboard = widgets.Output()\n",
    "    \n",
    "    # Filtres temporels\n",
    "    annees = sorted(df['Annee'].unique())\n",
    "    mois = list(range(1, 13))\n",
    "    mois_noms = ['Janvier', 'Février', 'Mars', 'Avril', 'Mai', 'Juin', \n",
    "                'Juillet', 'Août', 'Septembre', 'Octobre', 'Novembre', 'Décembre']\n",
    "    mois_dict = {i+1: nom for i, nom in enumerate(mois_noms)}\n",
    "    \n",
    "    # Filtres magnitude et profondeur\n",
    "    min_mag = float(df['Magnitude'].min())\n",
    "    max_mag = float(df['Magnitude'].max())\n",
    "    min_prof = float(df['Profondeur'].min())\n",
    "    max_prof = float(df['Profondeur'].max())\n",
    "    \n",
    "    # Filtres par catégories\n",
    "    mag_categories = sorted(df['Magnitude_Categorie'].unique())\n",
    "    prof_categories = sorted(df['Profondeur_Categorie'].unique())\n",
    "    pot_categories = sorted(df['Potentiel_Categorie'].unique())\n",
    "    \n",
    "    # Création des widgets\n",
    "    annee_slider = widgets.IntRangeSlider(\n",
    "        value=[annees[0], annees[-1]],\n",
    "        min=annees[0],\n",
    "        max=annees[-1],\n",
    "        step=1,\n",
    "        description='Années:',\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width='70%')\n",
    "    )\n",
    "    \n",
    "    mois_checkbox = widgets.SelectMultiple(\n",
    "        options=[(mois_dict[m], m) for m in mois],\n",
    "        value=mois,\n",
    "        description='Mois:',\n",
    "        layout=widgets.Layout(width='50%', height='100px')\n",
    "    )\n",
    "    \n",
    "    magnitude_slider = widgets.FloatRangeSlider(\n",
    "        value=[min_mag, max_mag],\n",
    "        min=min_mag,\n",
    "        max=max_mag,\n",
    "        step=0.1,\n",
    "        description='Magnitude:',\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width='70%')\n",
    "    )\n",
    "    \n",
    "    profondeur_slider = widgets.FloatRangeSlider(\n",
    "        value=[min_prof, max_prof],\n",
    "        min=min_prof,\n",
    "        max=max_prof,\n",
    "        step=5,\n",
    "        description='Profondeur:',\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width='70%')\n",
    "    )\n",
    "    \n",
    "    # Sélection multiple pour les catégories\n",
    "    magnitude_cat_select = widgets.SelectMultiple(\n",
    "        options=mag_categories,\n",
    "        value=mag_categories,\n",
    "        description='Cat. Magnitude:',\n",
    "        layout=widgets.Layout(width='50%', height='100px')\n",
    "    )\n",
    "    \n",
    "    profondeur_cat_select = widgets.SelectMultiple(\n",
    "        options=prof_categories,\n",
    "        value=prof_categories,\n",
    "        description='Cat. Profondeur:',\n",
    "        layout=widgets.Layout(width='50%', height='100px')\n",
    "    )\n",
    "    \n",
    "    potentiel_cat_select = widgets.SelectMultiple(\n",
    "        options=pot_categories,\n",
    "        value=pot_categories,\n",
    "        description='Cat. Potentiel:',\n",
    "        layout=widgets.Layout(width='50%', height='100px')\n",
    "    )\n",
    "    \n",
    "    # Types d'analyse\n",
    "    analyse_type = widgets.RadioButtons(\n",
    "        options=['Distribution des magnitudes', 'Distribution des profondeurs', \n",
    "                'Relation magnitude/profondeur', 'Potentiel destructeur', \n",
    "                'Énergie libérée', 'Analyse par clustering'],  # Ajout de l'option clustering\n",
    "        description='Type d\\'analyse:',\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "    \n",
    "    filtrer_button = widgets.Button(\n",
    "        description='Analyser caractéristiques',\n",
    "        button_style='primary',\n",
    "        tooltip='Cliquez pour analyser les caractéristiques',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    \n",
    "    # Fonction principale pour mettre à jour l'analyse\n",
    "    def update_dashboard(b):\n",
    "        with output_dashboard:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Récupérer les valeurs des filtres\n",
    "            annee_range = annee_slider.value\n",
    "            mois_selected = mois_checkbox.value\n",
    "            magnitude_range = magnitude_slider.value\n",
    "            profondeur_range = profondeur_slider.value\n",
    "            \n",
    "            # Récupérer les catégories sélectionnées\n",
    "            mag_cats_selected = magnitude_cat_select.value\n",
    "            prof_cats_selected = profondeur_cat_select.value\n",
    "            pot_cats_selected = potentiel_cat_select.value\n",
    "            \n",
    "            # Appliquer les filtres\n",
    "            df_filtered = appliquer_filtres(df, annee_range, mois_selected, magnitude_range, profondeur_range,\n",
    "                                         pot_cats_selected, mag_cats_selected, prof_cats_selected)\n",
    "            \n",
    "            print(f\"Données filtrées: {len(df_filtered)} séismes sur {len(df)} ({len(df_filtered)/len(df)*100:.1f}%)\")\n",
    "            \n",
    "            if len(df_filtered) == 0:\n",
    "                print(\"Aucune donnée ne correspond aux critères de filtrage!\")\n",
    "                return\n",
    "            \n",
    "            # Effectuer l'analyse sélectionnée\n",
    "            analysis_type = analyse_type.value\n",
    "            \n",
    "            if analysis_type == 'Distribution des magnitudes':\n",
    "                analyser_distribution_magnitudes(df_filtered)\n",
    "                \n",
    "            elif analysis_type == 'Distribution des profondeurs':\n",
    "                analyser_distribution_profondeurs(df_filtered)\n",
    "                \n",
    "            elif analysis_type == 'Relation magnitude/profondeur':\n",
    "                analyser_relation_magnitude_profondeur(df_filtered)\n",
    "                \n",
    "            elif analysis_type == 'Potentiel destructeur':\n",
    "                analyser_potentiel_destructeur(df_filtered)\n",
    "                \n",
    "            elif analysis_type == 'Énergie libérée':\n",
    "                analyser_energie(df_filtered)\n",
    "                \n",
    "            elif analysis_type == 'Analyse par clustering':\n",
    "                analyser_clustering(df_filtered)\n",
    "    \n",
    "    # Connecter le bouton à la fonction d'actualisation\n",
    "    filtrer_button.on_click(update_dashboard)\n",
    "    \n",
    "    # Afficher l'interface\n",
    "    print(\"\\n--- DASHBOARD D'ANALYSE DES CARACTÉRISTIQUES SISMIQUES ---\")\n",
    "    \n",
    "    # Organisation des filtres en accordéon\n",
    "    accordion = widgets.Accordion(\n",
    "        children=[\n",
    "            widgets.VBox([\n",
    "                widgets.HBox([annee_slider]),\n",
    "                widgets.HBox([mois_checkbox])\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                widgets.HBox([magnitude_slider]),\n",
    "                widgets.HBox([profondeur_slider])\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                widgets.HBox([magnitude_cat_select, profondeur_cat_select, potentiel_cat_select])\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "    accordion.set_title(0, 'Filtres temporels')\n",
    "    accordion.set_title(1, 'Filtres numériques')\n",
    "    accordion.set_title(2, 'Filtres catégoriels')\n",
    "    \n",
    "    controls = widgets.VBox([\n",
    "        accordion,\n",
    "        widgets.HBox([widgets.Label('Type d\\'analyse:', style={'font_weight': 'bold'})]),\n",
    "        widgets.HBox([analyse_type]),\n",
    "        widgets.HBox([filtrer_button])\n",
    "    ])\n",
    "    \n",
    "    display(controls)\n",
    "    display(output_dashboard)\n",
    "    \n",
    "    # Lancer l'analyse avec les filtres par défaut\n",
    "    update_dashboard(None)\n",
    "\n",
    "# Lancer le dashboard\n",
    "if __name__ == \"__main__\":\n",
    "    creer_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4aba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
